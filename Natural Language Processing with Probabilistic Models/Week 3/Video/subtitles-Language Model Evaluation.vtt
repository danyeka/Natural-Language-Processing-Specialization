WEBVTT

1
00:00:00.140 --> 00:00:04.440
In this video, I'll show you how
to evaluate a language model.

2
00:00:04.440 --> 00:00:09.140
The metric for this is called perplexity
and I'll explain what this is.

3
00:00:09.140 --> 00:00:13.710
First you will divide the tax corpus
into train validation and test data.

4
00:00:13.710 --> 00:00:17.070
Then you will dive into
the concept of perplexity.

5
00:00:17.070 --> 00:00:20.160
An important metric used to
evaluate language models.

6
00:00:20.160 --> 00:00:24.276
So how can you tell how well your
language model is performing?

7
00:00:24.276 --> 00:00:29.128
Recall from the previous videos that
a language model assigns a probability

8
00:00:29.128 --> 00:00:30.740
to each sentence.

9
00:00:30.740 --> 00:00:32.940
The model was trained on a corpus.

10
00:00:32.940 --> 00:00:37.260
So for the training sentences it
may assign very high probabilities.

11
00:00:37.260 --> 00:00:41.200
You should therefore first split
the corpus to have some testing and

12
00:00:41.200 --> 00:00:44.253
validation data that are not used for
the training.

13
00:00:44.253 --> 00:00:47.622
As you may have done in the other
machine learning projects,

14
00:00:47.622 --> 00:00:52.540
you will create the following split of
training, validation, and test sets.

15
00:00:52.540 --> 00:00:55.440
The training set is used
to train your model.

16
00:00:55.440 --> 00:01:00.205
The validation set is used for
things like tuning hyper parameters, and

17
00:01:00.205 --> 00:01:04.028
the test set is held out for
the end where you test it once and

18
00:01:04.028 --> 00:01:09.940
get an accuracy score that reflects how
well your model performs on unseen data.

19
00:01:09.940 --> 00:01:14.872
A commonly used split for
smaller datasets is 80, 10, 10 or

20
00:01:14.872 --> 00:01:19.819
80% of training,
10% of validation and 10% to testing.

21
00:01:19.819 --> 00:01:24.486
In a very large data set such as
tax analysis testing could make

22
00:01:24.486 --> 00:01:27.730
up as little as 1% of your training sets.

23
00:01:27.730 --> 00:01:32.340
In NLP there are two main methods for
splitting.

24
00:01:32.340 --> 00:01:37.219
You can split the corpus by choosing
longer continuous segments like

25
00:01:37.219 --> 00:01:38.817
Wikipedia articles or

26
00:01:38.817 --> 00:01:44.551
you can randomly choose short sequences
of words such as those in the sentences.

27
00:01:45.640 --> 00:01:47.686
Now that you've split the dataset,

28
00:01:47.686 --> 00:01:51.740
you can evaluate the test sets
using the perplexity metric.

29
00:01:51.740 --> 00:01:56.240
Perplexity is a commonly used
metric in language modeling.

30
00:01:56.240 --> 00:01:57.546
But what does it mean?

31
00:01:57.546 --> 00:02:01.576
If you're familiar with the word
perplexed you know that a person is

32
00:02:01.576 --> 00:02:05.640
perplexed when they are confused
by something very complex.

33
00:02:05.640 --> 00:02:11.357
You can think of perplexity as a measure
of the complexity in a sample of texts,

34
00:02:11.357 --> 00:02:13.625
like how complex that text is.

35
00:02:13.625 --> 00:02:18.800
Perplexity is used to tell us whether
a set of sentences look like they were

36
00:02:18.800 --> 00:02:25.040
written by humans rather than by a simple
program choosing words at random.

37
00:02:25.040 --> 00:02:30.261
A text that is written by humans is more
likely to have a lower perplexity score.

38
00:02:31.840 --> 00:02:33.000
On the other hand,

39
00:02:33.000 --> 00:02:38.140
a text generated by random more choice
would have a higher perplexity.

40
00:02:38.140 --> 00:02:41.640
Let me show you how to calculate
the perplexity of the model.

41
00:02:41.640 --> 00:02:47.015
You'll start by computing the probability
of all sentences in your test sets and

42
00:02:47.015 --> 00:02:51.340
then raise the probability
to the power of -1 over m.

43
00:02:51.340 --> 00:02:55.229
Perplexity is basically the inverse
probability of the test set

44
00:02:55.229 --> 00:02:58.940
normalized by the number
of words in the test set.

45
00:02:58.940 --> 00:03:03.688
So the higher the language model estimates
the probability of your test set,

46
00:03:03.688 --> 00:03:06.180
the lower the perplexity is going to be.

47
00:03:06.180 --> 00:03:08.385
As a side note worth mentioning,

48
00:03:08.385 --> 00:03:13.361
perplexity as closely related to entropy,
which measures uncertainty.

49
00:03:13.361 --> 00:03:18.221
Let's look at an example of two language
models that are going to return

50
00:03:18.221 --> 00:03:21.466
different probabilities for
your test sets W.

51
00:03:21.466 --> 00:03:25.940
There are 100 words in the test set,
so m is equal to 100.

52
00:03:25.940 --> 00:03:31.951
The first model returns a 0.9 probability
of the test set which is very high.

53
00:03:33.240 --> 00:03:37.040
This means that the first model
predicts your test sets very well.

54
00:03:37.040 --> 00:03:39.840
So the model is highly effective.

55
00:03:39.840 --> 00:03:42.556
As you can see the perplexity for
that model and

56
00:03:42.556 --> 00:03:44.861
test set is about one which is very low.

57
00:03:45.940 --> 00:03:50.461
The second model returns a very low
probability for your test sets,

58
00:03:50.461 --> 00:03:52.533
10 to the power of -250.

59
00:03:52.533 --> 00:03:54.574
For this model and test set,

60
00:03:54.574 --> 00:04:01.440
the perplexity is equal to about 316 which
is much higher than the first model.

61
00:04:01.440 --> 00:04:06.187
So one thing to remember is that
the smaller the perplexity score,

62
00:04:06.187 --> 00:04:10.696
the more likely the sentence is
to sound natural to human ears.

63
00:04:10.696 --> 00:04:15.437
For context, good language models
have perplexity scores between 60

64
00:04:15.437 --> 00:04:18.243
to20 sometimes even lower for English.

65
00:04:18.243 --> 00:04:22.825
Perplexities for character level
language models where you track

66
00:04:22.825 --> 00:04:26.340
characters instead of words will be lower.

67
00:04:26.340 --> 00:04:30.499
Now it gets ready to calculate
perplexity for bigram models and

68
00:04:30.499 --> 00:04:36.071
a bigram model you calculate the product
of bigram probabilities of all sentences,

69
00:04:36.071 --> 00:04:38.840
then take the power of -1 over m.

70
00:04:38.840 --> 00:04:44.565
Recall that the power of -1 over m of
the probability is the same as the mth or

71
00:04:44.565 --> 00:04:48.140
the roots of one over probability.

72
00:04:48.140 --> 00:04:53.206
One thing to notice here is that in the
case of the same probability for different

73
00:04:53.206 --> 00:04:58.366
test sets, the bigger the sets m is the
lower the final perplexity is going to be.

74
00:04:58.366 --> 00:05:02.775
If all sentences in the test set
are concatenated the formula can be

75
00:05:02.775 --> 00:05:08.440
simplified to the product of probabilities
of bigrams in the entire sets.

76
00:05:08.440 --> 00:05:12.787
One other thing to note is that some
papers use log perplexity instead of

77
00:05:12.787 --> 00:05:14.440
perplexity.

78
00:05:14.440 --> 00:05:20.392
So the perplexity formula changes from the
mth or the roots of one over probability

79
00:05:20.392 --> 00:05:26.740
to 1 over m times the sum of the
logarithms of the probabilities of words.

80
00:05:26.740 --> 00:05:28.362
This is easier to compute.

81
00:05:28.362 --> 00:05:33.136
So it's not uncommon to find researchers
reporting the log perplexity of

82
00:05:33.136 --> 00:05:34.369
language models.

83
00:05:34.369 --> 00:05:39.393
Note that the algorithm to the base two
is typically used in a good model with

84
00:05:39.393 --> 00:05:45.740
perplexity between 20 and 16 log
perplexity would be between 4.3 and 5.9.

85
00:05:45.740 --> 00:05:50.420
Now how does the improved perplexity
translate in a production quality

86
00:05:50.420 --> 00:05:51.591
language model?

87
00:05:51.591 --> 00:05:55.440
Here is an example of
a Wall Street Journal corpus.

88
00:05:55.440 --> 00:06:02.740
If you take a unigram language model,
the perplexity is very high, 962.

89
00:06:02.740 --> 00:06:06.140
This just generates words
by their probability.

90
00:06:06.140 --> 00:06:11.140
With a bigram language model, the text
starts to make a little more sense.

91
00:06:11.140 --> 00:06:16.333
Using a trigram you can see the language
it produces is pretty close to reasonable.

92
00:06:16.333 --> 00:06:18.845
The perplexity is now equal to 109,

93
00:06:18.845 --> 00:06:23.580
much closer to the target perplexity
of 20 to 60 I mentioned earlier.

94
00:06:23.580 --> 00:06:28.245
Later in the specialization you'll
encounter deep learning language

95
00:06:28.245 --> 00:06:31.287
models with even lower
perplexities course.

96
00:06:31.287 --> 00:06:36.236
You now understand what perplexity is and
how to evaluate language models.

97
00:06:36.236 --> 00:06:37.879
To use them for real tasks,

98
00:06:37.879 --> 00:06:42.840
we will need to be able to handle words
that did not occur in the training set.

99
00:06:42.840 --> 00:06:45.061
I'll show you how to do
that in the next video.