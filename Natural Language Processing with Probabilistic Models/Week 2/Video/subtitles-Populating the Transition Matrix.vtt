WEBVTT

1
00:00:00.000 --> 00:00:02.955
To populate the
transition matrix,

2
00:00:02.955 --> 00:00:04.050
you need to calculate

3
00:00:04.050 --> 00:00:06.645
the probability of a
tagging happening,

4
00:00:06.645 --> 00:00:09.195
given that's another
tag already happened.

5
00:00:09.195 --> 00:00:11.760
You also need to calculate
the probability that

6
00:00:11.760 --> 00:00:14.700
a tag happens at the
beginning of the sentence.

7
00:00:14.700 --> 00:00:16.830
You will also learn
about a new concept

8
00:00:16.830 --> 00:00:19.080
known as smoothing.
Let's get started.

9
00:00:19.080 --> 00:00:21.150
Begin by filling
the first column of

10
00:00:21.150 --> 00:00:24.630
your matrix with the counts
of the associated tags.

11
00:00:24.630 --> 00:00:27.570
Remember, the rows in
the matrix represent

12
00:00:27.570 --> 00:00:29.100
the current states and

13
00:00:29.100 --> 00:00:31.410
the columns represent
the next states.

14
00:00:31.410 --> 00:00:34.440
The values represents the
transition probabilities

15
00:00:34.440 --> 00:00:37.150
of going from the current
state to the next state.

16
00:00:37.150 --> 00:00:40.685
The states for this use case
is the parts of speech tag.

17
00:00:40.685 --> 00:00:43.220
As you can see, the
defined tags and

18
00:00:43.220 --> 00:00:44.870
the elements in the corpus are

19
00:00:44.870 --> 00:00:46.925
marked with
corresponding colors.

20
00:00:46.925 --> 00:00:48.350
For the first column,

21
00:00:48.350 --> 00:00:49.910
you'll count the occurrences of

22
00:00:49.910 --> 00:00:51.880
the following tag combinations.

23
00:00:51.880 --> 00:00:53.345
As you see here,

24
00:00:53.345 --> 00:00:57.515
a noun following a start token
occurs once in our corpus.

25
00:00:57.515 --> 00:01:00.800
A noun following a noun
doesn't occur at all.

26
00:01:00.800 --> 00:01:04.300
A noun following a verb
doesn't occur either.

27
00:01:04.300 --> 00:01:09.510
A noun following another
tag occurs six times.

28
00:01:09.510 --> 00:01:12.950
The rest of the matrix is
populated accordingly.

29
00:01:12.950 --> 00:01:15.155
But I'll take it a
little shortcut.

30
00:01:15.155 --> 00:01:17.135
The corpus, as you can see,

31
00:01:17.135 --> 00:01:19.145
is a verbalize high cool,

32
00:01:19.145 --> 00:01:21.450
because there are no
tag combinations with

33
00:01:21.450 --> 00:01:23.975
the tag VB, they are all 0.

34
00:01:23.975 --> 00:01:26.900
Unfortunately, there is
no such shortcuts in

35
00:01:26.900 --> 00:01:29.765
the programming assignments.
You've been warned.

36
00:01:29.765 --> 00:01:31.355
The 0 tag,

37
00:01:31.355 --> 00:01:35.395
the O tag following a
start token occurs twice.

38
00:01:35.395 --> 00:01:37.995
The O tag following a noun tag,

39
00:01:37.995 --> 00:01:40.410
NN occurs six times,

40
00:01:40.410 --> 00:01:44.090
and the last entry in the
transition matrix of an O tag,

41
00:01:44.090 --> 00:01:47.140
following an O tag
has a count of eight.

42
00:01:47.140 --> 00:01:49.790
In the last line, you
have to take into

43
00:01:49.790 --> 00:01:52.775
account the tagged words on a,

44
00:01:52.775 --> 00:01:56.845
a, wet, wet,

45
00:01:56.845 --> 00:02:01.635
and, back to calculate
the correct counts.

46
00:02:01.635 --> 00:02:04.070
Now, this you have calculated

47
00:02:04.070 --> 00:02:07.045
the counts of all tag
combinations in the matrix,

48
00:02:07.045 --> 00:02:09.890
you can calculate the
transition probabilities.

49
00:02:09.890 --> 00:02:12.140
So far, you've calculated

50
00:02:12.140 --> 00:02:14.270
then enter the counts
in the matrix,

51
00:02:14.270 --> 00:02:17.425
which corresponds to the
numerator in our formula.

52
00:02:17.425 --> 00:02:21.035
Now, you just have to divide
each counts by the sum,

53
00:02:21.035 --> 00:02:23.720
which is actually the
corresponding row sum.

54
00:02:23.720 --> 00:02:26.280
Remember what this
row sum represents.

55
00:02:26.280 --> 00:02:28.085
For the row where
the current state

56
00:02:28.085 --> 00:02:29.825
is a noun part of speech,

57
00:02:29.825 --> 00:02:32.105
this sum across
that row represents

58
00:02:32.105 --> 00:02:35.570
all pairs of words where the
current state is a noun,

59
00:02:35.570 --> 00:02:38.195
and the next state is
any parts of speech,

60
00:02:38.195 --> 00:02:40.840
whether it's a noun,
a verb, or other.

61
00:02:40.840 --> 00:02:42.890
For the transition
probability of

62
00:02:42.890 --> 00:02:44.870
a noun, tag NN

63
00:02:44.870 --> 00:02:47.780
Following a start token,
or in other words,

64
00:02:47.780 --> 00:02:50.510
the initial
probability of NN tag,

65
00:02:50.510 --> 00:02:52.375
we divide one by three,

66
00:02:52.375 --> 00:02:54.560
or four the transition
probability of

67
00:02:54.560 --> 00:02:57.080
an other tag followed
by a noun tag,

68
00:02:57.080 --> 00:02:59.210
we divide 6 by 14.

69
00:02:59.210 --> 00:03:02.540
You may have realized that
there are two problems here.

70
00:03:02.540 --> 00:03:06.050
One is that the row sum
of the VB tag is 0,

71
00:03:06.050 --> 00:03:09.605
which would lead to a division
by 0 using this formula.

72
00:03:09.605 --> 00:03:11.630
The other is that in lots of

73
00:03:11.630 --> 00:03:14.495
entries in the
transition matrix are 0,

74
00:03:14.495 --> 00:03:18.380
meaning that these transitions
will have probability 0.

75
00:03:18.380 --> 00:03:20.390
This won't work if
you want the model to

76
00:03:20.390 --> 00:03:22.460
generalize to other high cools,

77
00:03:22.460 --> 00:03:24.455
which might actually
contain verbs.

78
00:03:24.455 --> 00:03:27.500
To handle this, change
your formula slightly

79
00:03:27.500 --> 00:03:30.740
by adding a small
value Epsilon to each

80
00:03:30.740 --> 00:03:33.335
of the counts in the
numerator and add

81
00:03:33.335 --> 00:03:34.610
N times Epsilon to

82
00:03:34.610 --> 00:03:38.305
the divisor such that the row
sums still adds up to one.

83
00:03:38.305 --> 00:03:41.705
This operation is also
referred to as smoothing,

84
00:03:41.705 --> 00:03:44.360
which you might remember
from previous lessons.

85
00:03:44.360 --> 00:03:47.405
If you substitute the
Epsilon with a small value,

86
00:03:47.405 --> 00:03:50.150
say 0.001, then you'll

87
00:03:50.150 --> 00:03:52.360
get the following
transition matrix.

88
00:03:52.360 --> 00:03:54.410
The value shown here are

89
00:03:54.410 --> 00:03:56.795
shown up to the third
decimal digits,

90
00:03:56.795 --> 00:03:58.130
so don't worry, if

91
00:03:58.130 --> 00:04:00.875
the row sums don't add
up to one exactly.

92
00:04:00.875 --> 00:04:03.650
The results of smoothing
is, as you can see,

93
00:04:03.650 --> 00:04:07.460
that you no longer have
any 0 value entries in a.

94
00:04:07.460 --> 00:04:10.940
Further, since the transition
probabilities from

95
00:04:10.940 --> 00:04:12.950
the VB states are actually

96
00:04:12.950 --> 00:04:15.575
one-third for all
outgoing transitions,

97
00:04:15.575 --> 00:04:17.235
they are equally likely.

98
00:04:17.235 --> 00:04:19.550
That's reasonable.
Since you didn't have

99
00:04:19.550 --> 00:04:22.535
any data to estimate these
transition probabilities.

100
00:04:22.535 --> 00:04:24.370
One more thing before you go,

101
00:04:24.370 --> 00:04:26.314
and a real-world example,

102
00:04:26.314 --> 00:04:28.460
you might not want to
apply smoothing to

103
00:04:28.460 --> 00:04:30.080
the initial probabilities in

104
00:04:30.080 --> 00:04:32.720
the first row of the
transition matrix.

105
00:04:32.720 --> 00:04:36.440
That's because if you apply
smoothing to that row by

106
00:04:36.440 --> 00:04:40.085
adding a small value to
possibly zeroed valued entries.

107
00:04:40.085 --> 00:04:42.800
You'll effectively allow
a sentence to start with

108
00:04:42.800 --> 00:04:46.495
any parts of speech tag,
including punctuation.

109
00:04:46.495 --> 00:04:49.140
You just learned about
smoothing and so

110
00:04:49.140 --> 00:04:51.825
why it is important.
This is great.

111
00:04:51.825 --> 00:04:53.090
Now in the next video,

112
00:04:53.090 --> 00:04:55.670
we will move on and see
how you can populate

113
00:04:55.670 --> 00:05:00.149
another type of matrix known
as the emissions matrix.