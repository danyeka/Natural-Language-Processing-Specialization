WEBVTT

1
00:00:00.000 --> 00:00:02.987
You have learned all the basics
of attention by now.

2
00:00:02.987 --> 00:00:06.785
In fact, you could already
build a transformer from it.

3
00:00:06.785 --> 00:00:11.063
But if you want it to work really well,
run fast and get very good results,

4
00:00:11.063 --> 00:00:12.795
you'll need one more thing.

5
00:00:12.795 --> 00:00:14.680
The multi-head attention.

6
00:00:14.680 --> 00:00:16.925
Let me show you what it is.

7
00:00:16.925 --> 00:00:20.928
First, I'll share with you some
intuition on multi-head attention.

8
00:00:20.928 --> 00:00:24.131
Afterwards, I'll show
you the math behind it.

9
00:00:24.131 --> 00:00:27.444
Recall that you need word embeddings for
the query key and

10
00:00:27.444 --> 00:00:30.489
value matrices in scaled
dot-product attention.

11
00:00:30.489 --> 00:00:32.174
In multi-head attention,

12
00:00:32.174 --> 00:00:36.385
you apply in parallel the attention
mechanism to multiple sets of these

13
00:00:36.385 --> 00:00:40.469
matrices that you can get by
transforming the original embeddings.

14
00:00:40.469 --> 00:00:44.862
In multi-head attention, the number
of times that you apply the attention

15
00:00:44.862 --> 00:00:47.548
mechanism is the number
of heads in the model.

16
00:00:47.548 --> 00:00:51.174
For instance, you will need
two sets of queries, keys and

17
00:00:51.174 --> 00:00:53.251
values in a model with two heads.

18
00:00:53.251 --> 00:00:56.837
The first head would use
a set of representations and

19
00:00:56.837 --> 00:00:59.781
the second head would use a different set.

20
00:00:59.781 --> 00:01:04.776
In the transformer model,
you'd get different representations

21
00:01:04.776 --> 00:01:09.593
by linearly transforming the original
embeddings by using a set

22
00:01:09.593 --> 00:01:13.984
of matrices W superscripts QKV for
each head in the model.

23
00:01:13.984 --> 00:01:18.406
Using different sets of representations,
allow your model to learn

24
00:01:18.406 --> 00:01:23.304
multiple relationships between the words
from the query and key matrices.

25
00:01:23.304 --> 00:01:27.765
With that in mind, let me show you
how multi-head attention works.

26
00:01:27.765 --> 00:01:33.309
The inputs to multi-head detention
is the value key and query matrices.

27
00:01:33.309 --> 00:01:38.447
First, you transform each of these
matrices into multiple vector spaces.

28
00:01:38.447 --> 00:01:42.644
As you saw previously,
the number of transformations for

29
00:01:42.644 --> 00:01:46.677
each matrix is equal to
the number of heads in the model.

30
00:01:46.677 --> 00:01:51.707
Then you will apply the scaled
dot-product attention mechanism

31
00:01:51.707 --> 00:01:56.004
to every set of value, key and
query transformations,

32
00:01:56.004 --> 00:02:01.687
where again the number of sets is equal
to the number of heads in the model.

33
00:02:01.687 --> 00:02:06.302
After that, you concatenate
the results from each head in

34
00:02:06.302 --> 00:02:08.852
the model into a single matrix.

35
00:02:08.852 --> 00:02:14.810
Finally, you transform the resulting
matrix to get the output context vectors.

36
00:02:14.810 --> 00:02:19.703
Note that every linear transformation
in multi-head attention contains a set

37
00:02:19.703 --> 00:02:24.312
of learnable parameters, so let's go
through every step in more detail.

38
00:02:24.312 --> 00:02:27.042
Say that you have two heads in your model.

39
00:02:27.042 --> 00:02:31.777
The inputs to the multi-head attention
layer are the queries, keys, and

40
00:02:31.777 --> 00:02:32.990
values matrices.

41
00:02:32.990 --> 00:02:37.299
The number of columns in those
matrices is equal to d sub model,

42
00:02:37.299 --> 00:02:42.258
which is the embedding size, and
the number of rows is given by the number

43
00:02:42.258 --> 00:02:46.029
of words the sequences used
to construct each matrix.

44
00:02:46.029 --> 00:02:50.970
The first step is to transform the
queries, keys and values using a set of

45
00:02:50.970 --> 00:02:54.858
matrices W superscripts, QK and
V per head of the model.

46
00:02:54.858 --> 00:02:59.837
This step will give you the different
sets of representations that you use for

47
00:02:59.837 --> 00:03:02.296
the parallel attention mechanisms.

48
00:03:02.296 --> 00:03:07.562
The number of rows and the transformation
matrices is equal to d sub model.

49
00:03:07.562 --> 00:03:10.955
The number of columns d sub K for
the queries and

50
00:03:10.955 --> 00:03:15.914
keys transformation matrices, and
the number of columns d sub V for

51
00:03:15.914 --> 00:03:20.177
W superscript V are hyperparameters
that you can choose.

52
00:03:20.177 --> 00:03:22.499
|n the original transformer model,

53
00:03:22.499 --> 00:03:25.686
the author advises setting d sub K and
d sub V equals to

54
00:03:25.686 --> 00:03:30.350
the dimension of the embeddings divided
by the number of heads in the model.

55
00:03:30.350 --> 00:03:34.970
This choice of sizes would ensure that
the computational cost of multi head

56
00:03:34.970 --> 00:03:39.388
attention doesn't exceed by much
the one for a single head attention.

57
00:03:39.388 --> 00:03:44.335
After getting the transformed values for
the query key and value matrices per head,

58
00:03:44.335 --> 00:03:47.414
you can apply in parallel
the attention mechanism.

59
00:03:47.414 --> 00:03:52.355
As a result, you get a matrix per head
with the column dimensions equal to d sub

60
00:03:52.355 --> 00:03:57.068
V, and the number of rows in those
matrices is the same as the number of rows

61
00:03:57.068 --> 00:03:58.455
in the query matrix.

62
00:03:58.455 --> 00:04:02.826
Then you concatenate horizontally
the matrices outputted by each

63
00:04:02.826 --> 00:04:04.783
attention head in the model.

64
00:04:04.783 --> 00:04:11.925
So you will get a matrix that has d sub
V times the number of heads columns.

65
00:04:11.925 --> 00:04:18.702
Then you apply a linear transformation W
superscript O to the concatenated matrix.

66
00:04:18.702 --> 00:04:23.392
This linear transformation has
columns equal to d sub model, and

67
00:04:23.392 --> 00:04:28.592
if you choose d sub V to be equal to
the embedding size divided by the number

68
00:04:28.592 --> 00:04:33.559
of heads, the number of rows in this
matrix would also be d sub model.

69
00:04:33.559 --> 00:04:38.164
Just as with single head attention,
you will get a matrix with the context

70
00:04:38.164 --> 00:04:42.034
vectors of size d sub model for
each of your original queries.

71
00:04:42.034 --> 00:04:47.558
That's it for multi-head attention, you
just need to apply the attention mechanism

72
00:04:47.558 --> 00:04:52.339
to multiple sets of representations for
the queries, keys, and values.

73
00:04:52.339 --> 00:04:57.480
Then you concatenate the results from each
attention computation to get a matrix

74
00:04:57.480 --> 00:05:02.630
that you linearly transform to get the
context vectors for each original query.

75
00:05:02.630 --> 00:05:06.650
In this video, you learned how
multi-head attention works and you saw

76
00:05:06.650 --> 00:05:11.214
some of the dimensions of the parameter
matrices involved in its calculations.

77
00:05:11.214 --> 00:05:15.334
You can implement multi-head attention
to make computations in parallel.

78
00:05:15.334 --> 00:05:19.417
And with a proper choice of sizes for
the transformation matrices,

79
00:05:19.417 --> 00:05:24.087
the total computational time is similar
to the one of single head attention.

80
00:05:24.087 --> 00:05:27.431
In the last three videos,
you learned about attention.

81
00:05:27.431 --> 00:05:32.711
You know the basic dot product attention,
the causal one, and the multi-headed one.

82
00:05:32.711 --> 00:05:35.824
You're now ready to build
your own transformer decoder.

83
00:05:35.824 --> 00:05:37.460
That's what we'll do in the next video.