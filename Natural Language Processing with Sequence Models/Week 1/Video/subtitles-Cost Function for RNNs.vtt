WEBVTT

1
00:00:00.000 --> 00:00:02.775
We will now look at the
cost function for RNNs.

2
00:00:02.775 --> 00:00:05.325
We will go over the cross
entropy loss again,

3
00:00:05.325 --> 00:00:06.900
and see how we can adapt it,

4
00:00:06.900 --> 00:00:09.240
so we can account for
all the time steps.

5
00:00:09.240 --> 00:00:13.485
Let's get started. Take a
look at the sequential model.

6
00:00:13.485 --> 00:00:17.794
Here you have an input
vector x in magenta,

7
00:00:17.794 --> 00:00:19.835
multiple hidden units in blue,

8
00:00:19.835 --> 00:00:22.250
and three outputs
units in green.

9
00:00:22.250 --> 00:00:25.985
Suppose that this model
outputs the probability

10
00:00:25.985 --> 00:00:27.920
of whether an
observation belongs

11
00:00:27.920 --> 00:00:30.130
to each of three
different classes.

12
00:00:30.130 --> 00:00:33.005
Looking at a single x, y pair,

13
00:00:33.005 --> 00:00:36.050
you can get the loss using
the cross entropy function,

14
00:00:36.050 --> 00:00:39.560
where k is the number of
categories or classes.

15
00:00:39.560 --> 00:00:45.570
Every label y_j is
equal to either 1 or 0.

16
00:00:45.570 --> 00:00:48.935
To get the loss for a
single observation,

17
00:00:48.935 --> 00:00:53.030
you have to look at every
outputs probability in y hat

18
00:00:53.030 --> 00:00:57.535
and compare them with the
true y values 0 or 1.

19
00:00:57.535 --> 00:01:00.380
Vanilla RNNs have
an architecture

20
00:01:00.380 --> 00:01:03.815
similar to this one where
for every time step t,

21
00:01:03.815 --> 00:01:06.625
the network outputs
a vector y hat.

22
00:01:06.625 --> 00:01:09.765
By computing these two
formulas at every step,

23
00:01:09.765 --> 00:01:11.240
the cross entropy loss in

24
00:01:11.240 --> 00:01:13.400
this case is shown
in this equation.

25
00:01:13.400 --> 00:01:16.580
Where the only difference
from the equation that I

26
00:01:16.580 --> 00:01:20.335
showed earlier is this
summation over time t,

27
00:01:20.335 --> 00:01:23.315
and a division by the
total number of steps,

28
00:01:23.315 --> 00:01:25.310
capital T. This is

29
00:01:25.310 --> 00:01:26.720
just the average of

30
00:01:26.720 --> 00:01:29.795
the cross entropy loss
function over time.

31
00:01:29.795 --> 00:01:32.870
To get the cost of your
whole model, as always,

32
00:01:32.870 --> 00:01:36.410
you would have to sum over
every example in the training,

33
00:01:36.410 --> 00:01:38.380
development, or test set.

34
00:01:38.380 --> 00:01:40.340
Now you know that to capture

35
00:01:40.340 --> 00:01:42.455
an individual loss
in your model,

36
00:01:42.455 --> 00:01:44.660
you only have to take
the average with

37
00:01:44.660 --> 00:01:47.770
respect to time for your
recurrent neural network.

38
00:01:47.770 --> 00:01:51.155
Next, I'll share with you
some implementation notes,

39
00:01:51.155 --> 00:01:54.660
and the more complex
RNN architectures.

40
00:01:54.670 --> 00:01:58.355
You have seen how we change
the cost function by summing

41
00:01:58.355 --> 00:02:01.670
over all the k classes
and all the t time steps.

42
00:02:01.670 --> 00:02:03.740
We then divide it by
the total number of

43
00:02:03.740 --> 00:02:06.620
steps to get the average
cost in each step.

44
00:02:06.620 --> 00:02:08.375
We will further explore how to

45
00:02:08.375 --> 00:02:11.580
implement these models
in the next video.