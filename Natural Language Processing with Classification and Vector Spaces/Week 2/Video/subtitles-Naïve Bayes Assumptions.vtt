WEBVTT

1
00:00:00.000 --> 00:00:03.836
Now I'm going to the assumptions
underlying the naïve bayes method.

2
00:00:03.836 --> 00:00:06.983
The main one,
is independence of words in a sentence.

3
00:00:06.983 --> 00:00:12.131
And I'll tell you why this can be a big
problem, when the method is applied.

4
00:00:12.131 --> 00:00:17.113
Naïve bayes is a very simple model because
it doesn't require setting any custom

5
00:00:17.113 --> 00:00:18.001
parameters.

6
00:00:18.001 --> 00:00:20.225
This method is referred to as naïve,

7
00:00:20.225 --> 00:00:23.397
because of the assumptions
it makes about the data.

8
00:00:23.397 --> 00:00:27.293
The first assumption is independence
between the predictors or

9
00:00:27.293 --> 00:00:29.681
features associated with each class.

10
00:00:29.681 --> 00:00:33.204
And the second,
has to do with your validation set.

11
00:00:33.204 --> 00:00:37.258
Let's explore each of these assumptions
and how they could affect your results.

12
00:00:37.258 --> 00:00:40.596
To illustrate what independence
between features looks like,

13
00:00:40.596 --> 00:00:42.706
let's look at the following sentence.

14
00:00:42.706 --> 00:00:47.041
It is sunny and hot in the Sahara desert.
Naïve Bayes assumes that the words in

15
00:00:47.041 --> 00:00:52.578
a piece of text are independent
of one another.

16
00:00:52.578 --> 00:00:56.143
But as you can see,
this typically isn't the case,

17
00:00:56.143 --> 00:01:01.016
the word sunny and hot often appear
together as they do in this example.

18
00:01:01.016 --> 00:01:05.919
Taken together, they might also be
related to the thing they're describing,

19
00:01:05.919 --> 00:01:07.536
like a beach or a desert.

20
00:01:07.536 --> 00:01:11.861
So the words in the sentence are not
always necessarily independent of one

21
00:01:11.861 --> 00:01:12.499
another.

22
00:01:12.499 --> 00:01:16.952
But naïve bayes, assumes that they are,
this could lead you to under or

23
00:01:16.952 --> 00:01:21.275
overestimate the conditional
probabilities of individual words.

24
00:01:21.275 --> 00:01:23.947
When using a naïve bayes, for example,

25
00:01:23.947 --> 00:01:29.305
if your task was to complete the sentence,
it's always cold and snowy in blank.

26
00:01:29.305 --> 00:01:33.736
Naïve bayes might assign equal
probability to the words spring,

27
00:01:33.736 --> 00:01:36.404
summer, fall and winter.

28
00:01:36.404 --> 00:01:40.766
Even though from the context you can see
that winter should be the most likely

29
00:01:40.766 --> 00:01:45.220
candidate And the next courses of
this specialization, you will be

30
00:01:45.220 --> 00:01:49.333
introduced to some more sophisticated
methods, let's deal with this.

31
00:01:49.333 --> 00:01:53.949
Another issue with naïve bayes is
that it relies on the distribution of

32
00:01:53.949 --> 00:01:55.601
the training data sets.

33
00:01:55.601 --> 00:01:59.806
A good data set, will contain
the same proportion of positive and

34
00:01:59.806 --> 00:02:02.185
negative tweets as a random sample.

35
00:02:02.185 --> 00:02:07.639
However, most of the available annotated
corporal are artificially balanced,

36
00:02:07.639 --> 00:02:10.967
just like the data set you use for
the assignment.

37
00:02:10.967 --> 00:02:12.538
And the real tweet stream,

38
00:02:12.538 --> 00:02:16.994
positive tweets tend to occur more
often than their negative counterparts.

39
00:02:16.994 --> 00:02:20.035
One reason for
this is that negative tweets,

40
00:02:20.035 --> 00:02:25.009
might contain content that is banned
by the platform or muted by the user.

41
00:02:25.009 --> 00:02:28.376
Such as inappropriate or
offensive vocabulary,

42
00:02:28.376 --> 00:02:32.231
assuming that reality behaves
as your training corpus.

43
00:02:32.231 --> 00:02:36.696
This could result in a very optimistic or
very pessimistic model.

44
00:02:36.696 --> 00:02:41.016
There's a lot more on this, in the last
video of this module which analyzes

45
00:02:41.016 --> 00:02:43.226
the sources of errors in naïve bayes.

46
00:02:43.226 --> 00:02:46.325
Let's do a quick recap of
all this new information,

47
00:02:46.325 --> 00:02:51.127
the assumption of independence and naïve
bayes is very difficult to guarantee.

48
00:02:51.127 --> 00:02:55.283
But despite that, the model works
pretty well in certain situations.

49
00:02:55.283 --> 00:02:59.534
And for the assignments in this module,
the relative frequency of positive and

50
00:02:59.534 --> 00:03:02.010
negative tweets and
your training data sets,

51
00:03:02.010 --> 00:03:05.317
needs to be balanced in order
to deliver an accurate results.

52
00:03:05.317 --> 00:03:09.215
Now you understand the assumptions
that underlie the naïve bayes method.

53
00:03:09.215 --> 00:03:13.727
What if it fails to perform well,
for some sentence in the next video,

54
00:03:13.727 --> 00:03:16.261
I'll show you what to do in such cases.