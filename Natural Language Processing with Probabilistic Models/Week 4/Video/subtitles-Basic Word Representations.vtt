WEBVTT

1
00:00:00.000 --> 00:00:05.344
You will now be able to create a matrix to
present all the words in your vocabulary.

2
00:00:05.344 --> 00:00:09.112
Each vector of your matrix will
correspond to one of the words.

3
00:00:09.112 --> 00:00:11.810
Let me show you how you
can build this matrix.

4
00:00:11.810 --> 00:00:15.496
>> The simplest way to represent
words as numbers is for

5
00:00:15.496 --> 00:00:19.703
a given vocabulary to assign
a unique integer to each word.

6
00:00:19.703 --> 00:00:25.131
For example, based on a vocabulary
of 1000 basic english words,

7
00:00:25.131 --> 00:00:28.535
you could assign the number
1 to the word A,

8
00:00:28.535 --> 00:00:31.960
2 to able and so on up to 1000 for zebra.

9
00:00:31.960 --> 00:00:34.306
While this numbering scheme is simple,

10
00:00:34.306 --> 00:00:37.894
one of the problems is that
the order of the words alphabetical

11
00:00:37.894 --> 00:00:42.044
in this example doesn't make much
sense from a semantic perspective.

12
00:00:42.044 --> 00:00:46.927
For example, there is no reason why
happy should be assigned a greater

13
00:00:46.927 --> 00:00:51.005
number than hand for instance or
a smaller one than zebra.

14
00:00:51.005 --> 00:00:54.593
Instead of using a numerical
index to encode each word.

15
00:00:54.593 --> 00:00:57.767
You can represent the words
using a column vector where

16
00:00:57.767 --> 00:01:01.540
each element corresponds to
a word of the vocabulary.

17
00:01:01.540 --> 00:01:05.704
Using the previous example with
a vocabulary of 1000 words.

18
00:01:05.704 --> 00:01:09.091
Each vector would contain
1000 elements and

19
00:01:09.091 --> 00:01:12.401
each row would be labeled
with one of the words.

20
00:01:12.401 --> 00:01:17.304
You can now encode each word with a unique
column vector by putting A 1 in the row

21
00:01:17.304 --> 00:01:20.663
that has the same label,
and a zero everywhere else.

22
00:01:20.663 --> 00:01:25.541
For example, the word happy would
be represented as a vector with A 1

23
00:01:25.541 --> 00:01:30.334
in the row corresponding to happy,
and 0 in all of the other rows and

24
00:01:30.334 --> 00:01:33.382
for all the other words in the vocabulary.

25
00:01:33.382 --> 00:01:36.676
I'll call these vectors one
hot vectors to distinguish

26
00:01:36.676 --> 00:01:40.120
them from other types of
vectors that you'll encounter.

27
00:01:40.120 --> 00:01:43.564
You can consider words as
a categorical variable.

28
00:01:43.564 --> 00:01:46.454
You can easily go from integers
to one hot vectors and

29
00:01:46.454 --> 00:01:51.001
backed by simply mapping the words in the
rows to their corresponding row number.

30
00:01:51.001 --> 00:01:56.249
In this way, happy word number
621 would be represented as A 1

31
00:01:56.249 --> 00:02:02.103
hot vector with A 1 in row 621 which
corresponds to the word happy.

32
00:02:02.103 --> 00:02:07.356
Conversely, the vector for
happy with A 1 in row 621 would

33
00:02:07.356 --> 00:02:12.722
be represented by integer 621
which corresponds to happy.

34
00:02:12.722 --> 00:02:16.007
One hot vectors have
an advantage over using integers

35
00:02:16.007 --> 00:02:20.763
because one hot vectors do not imply
any relationship between any two words.

36
00:02:20.763 --> 00:02:24.694
Each vector says the word is
either happy or it's not,

37
00:02:24.694 --> 00:02:27.603
or the word is either zebra or it's not.

38
00:02:27.603 --> 00:02:33.303
However, one hot vectors have two major
limitations for most NLP use cases.

39
00:02:33.303 --> 00:02:37.208
For one, barring anything but
the simplest vocabulary,

40
00:02:37.208 --> 00:02:40.003
the vectors are going to be huge.

41
00:02:40.003 --> 00:02:44.873
This means that working with one hot
vectors on a computer would require a lot

42
00:02:44.873 --> 00:02:46.862
of space and processing time.

43
00:02:46.862 --> 00:02:49.814
If your vectors are created
with english words,

44
00:02:49.814 --> 00:02:52.701
you could end up with
upwards of a million rows.

45
00:02:52.701 --> 00:02:56.363
One row for
each word in the english vocabulary.

46
00:02:56.363 --> 00:03:00.425
Another limitation is that this
representation doesn't carry the word's

47
00:03:00.425 --> 00:03:01.001
meaning.

48
00:03:01.001 --> 00:03:05.852
For instance, if you attempt to determine
how similar two words are by calculating

49
00:03:05.852 --> 00:03:08.486
the distance between
their one hot vectors,

50
00:03:08.486 --> 00:03:12.802
then you will always get the same
distance between any two pairs of words.

51
00:03:12.802 --> 00:03:15.437
For example, using one hot vectors,

52
00:03:15.437 --> 00:03:20.804
the word happy is just as similar to the
word paper as it is to the word excited.

53
00:03:20.804 --> 00:03:25.767
Intuitively, you would think that happy
is more similar to excited than it

54
00:03:25.767 --> 00:03:26.644
is to paper.

55
00:03:26.644 --> 00:03:30.552
This is where word embeddings come
into play as I'll explain next.

56
00:03:30.552 --> 00:03:33.672
>> To recap,
you saw the pros of one hot vectors,

57
00:03:33.672 --> 00:03:37.603
which are pretty simple and
require no implied ordering.

58
00:03:37.603 --> 00:03:42.765
You also saw the cons of one hot vectors,
which are huge and encode no meaning.

59
00:03:42.765 --> 00:03:45.723
In the next video,
I will show you word embeddings,

60
00:03:45.723 --> 00:03:47.951
which will help us solve these issues.