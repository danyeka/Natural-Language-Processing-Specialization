WEBVTT

1
00:00:00.240 --> 00:00:01.490
Welcome back.

2
00:00:01.490 --> 00:00:05.973
Sometimes you try to calculate
the probability of a word happening after

3
00:00:05.973 --> 00:00:06.493
a word.

4
00:00:06.493 --> 00:00:11.000
To do that you might want to count the
number of times those two words showed up.

5
00:00:11.000 --> 00:00:15.740
One after another, divided by the number
of times the first word appeared.

6
00:00:15.740 --> 00:00:20.848
Now what if the two words never showed up
next to each other in the training corpus.

7
00:00:20.848 --> 00:00:22.909
You get a probability of zero, and

8
00:00:22.909 --> 00:00:26.740
the probability of an entire
sequence might go to zero.

9
00:00:26.740 --> 00:00:28.940
Now how can we fix this problem?

10
00:00:28.940 --> 00:00:33.055
Let's take a look,
let's now dive into Laplacian smoothing,

11
00:00:33.055 --> 00:00:38.140
a technique you can use to avoid
your probabilities being zero.

12
00:00:38.140 --> 00:00:42.619
The expression used to calculate
the conditional probability

13
00:00:42.619 --> 00:00:47.361
of a word given the class is
the frequency of the word in the corpus.

14
00:00:47.361 --> 00:00:52.334
Shown here as freq of word I comma
class divided by the number of words

15
00:00:52.334 --> 00:00:54.326
in the corpus or end class.

16
00:00:54.326 --> 00:00:57.428
Smoothing the probability
function means that you will

17
00:00:57.428 --> 00:01:00.402
use a slightly different
formula from the original.

18
00:01:00.402 --> 00:01:03.440
Know that I've added
a one in the numerator.

19
00:01:03.440 --> 00:01:07.240
This little transformation avoids
the probability being zero.

20
00:01:07.240 --> 00:01:11.290
However, it as a new term to
all the frequencies that is not

21
00:01:11.290 --> 00:01:14.340
correctly normalized by N class.

22
00:01:14.340 --> 00:01:20.140
To account for this, you will add
a new term in the denominator V class.

23
00:01:20.140 --> 00:01:24.377
That is the number of unique words
in your vocabulary to account for

24
00:01:24.377 --> 00:01:27.440
that extra term added in the numerator.

25
00:01:27.440 --> 00:01:31.340
So now all the probabilities in
each column will sum to one.

26
00:01:31.340 --> 00:01:33.651
This process is called
Laplacian smoothing.

27
00:01:34.940 --> 00:01:39.140
Going back to this example,
let's use the formula on it.

28
00:01:39.140 --> 00:01:42.565
The first thing you need to calculate
is the number of unique words in your

29
00:01:42.565 --> 00:01:43.940
vocabulary.

30
00:01:43.940 --> 00:01:46.740
In this example you have
eight unique words.

31
00:01:46.740 --> 00:01:50.140
So now let's calculate the probability for
each word.

32
00:01:50.140 --> 00:01:55.949
And the positive class for
the word I the positive class you get

33
00:01:55.949 --> 00:02:02.640
three plus one divided by 13
plus eight which is 0.19.

34
00:02:02.640 --> 00:02:08.386
For the negative class you have three
plus one divided by 12 plus eight,

35
00:02:08.386 --> 00:02:13.240
which is 0.2 and then so
on for the rest of the table.

36
00:02:13.240 --> 00:02:15.821
The number is shown
here have been rounded.

37
00:02:15.821 --> 00:02:20.628
But using this method to sum of
probabilities in your table will still be,

38
00:02:20.628 --> 00:02:25.440
one knows that the word because no
longer has a probability of zero.

39
00:02:25.440 --> 00:02:28.340
Great, that's Laplacian smoothing.

40
00:02:28.340 --> 00:02:31.070
Now you know why you have to
use the Laplacian smoothing.

41
00:02:31.070 --> 00:02:34.440
So your probabilities
don't end up being zero.

42
00:02:34.440 --> 00:02:38.708
That's very cool, you learned
about Laplacian smoothing and

43
00:02:38.708 --> 00:02:42.209
now understand why it is
important to use smoothing.

44
00:02:42.209 --> 00:02:46.060
In the next video,
you will learn about the log likelihood