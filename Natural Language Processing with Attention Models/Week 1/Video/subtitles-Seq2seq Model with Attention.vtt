WEBVTT

1
00:00:00.000 --> 00:00:04.065
Welcome. Attention is a
very important concepts and

2
00:00:04.065 --> 00:00:06.150
allows you to focus
where the model is

3
00:00:06.150 --> 00:00:08.745
looking at whenever
making a prediction.

4
00:00:08.745 --> 00:00:10.605
For example, when translating

5
00:00:10.605 --> 00:00:12.690
one paragraph from
English to French,

6
00:00:12.690 --> 00:00:14.385
you can focus on translating

7
00:00:14.385 --> 00:00:16.905
one sentence at a
time or even more,

8
00:00:16.905 --> 00:00:18.930
a couple of words at a time.

9
00:00:18.930 --> 00:00:21.245
Let's dive into this concept.

10
00:00:21.245 --> 00:00:24.450
What we call attention
now was introduced in

11
00:00:24.450 --> 00:00:27.495
a landmark paper from
Dzmitry Bahdanau,

12
00:00:27.495 --> 00:00:30.150
KyungHyun Cho, and
Yoshua Bengio.

13
00:00:30.150 --> 00:00:32.070
The authors developed a method

14
00:00:32.070 --> 00:00:33.690
to fix the seek to seek models,

15
00:00:33.690 --> 00:00:36.435
and ability to translate
longer sentences.

16
00:00:36.435 --> 00:00:38.790
As you can see, attention was

17
00:00:38.790 --> 00:00:41.175
originally developed for
machine translation,

18
00:00:41.175 --> 00:00:42.920
but it's since being used in

19
00:00:42.920 --> 00:00:45.200
many other domains
with great success.

20
00:00:45.200 --> 00:00:46.820
Before we move forward,

21
00:00:46.820 --> 00:00:48.860
I want to skip ahead
a bit and show

22
00:00:48.860 --> 00:00:50.995
you how well attention works.

23
00:00:50.995 --> 00:00:52.680
It's surprising.

24
00:00:52.680 --> 00:00:55.430
Here's a comparison of
the performance between

25
00:00:55.430 --> 00:00:57.980
different models from
the Bahdanau paper

26
00:00:57.980 --> 00:00:59.705
using the bleu score,

27
00:00:59.705 --> 00:01:02.860
a performance metric that
you'll learn about later.

28
00:01:02.860 --> 00:01:05.810
In brief, higher
scores are better,

29
00:01:05.810 --> 00:01:08.660
indicating more
correct translations.

30
00:01:08.660 --> 00:01:11.030
The dashed lines, they

31
00:01:11.030 --> 00:01:12.890
showed the scores for
bidirectional seek

32
00:01:12.890 --> 00:01:14.360
to seek model as

33
00:01:14.360 --> 00:01:16.750
the length of the input
sentence is increased.

34
00:01:16.750 --> 00:01:18.590
The 30 and 50 denotes

35
00:01:18.590 --> 00:01:21.950
the maximum sequence length
used to train the models.

36
00:01:21.950 --> 00:01:25.340
As you can see, the seek
to seek models perform

37
00:01:25.340 --> 00:01:28.595
welfare sentences with
about 10-20 words,

38
00:01:28.595 --> 00:01:30.730
but they fall off beyond that.

39
00:01:30.730 --> 00:01:32.625
This is what you should expect.

40
00:01:32.625 --> 00:01:34.880
A seek to seek models
must store the meaning of

41
00:01:34.880 --> 00:01:38.200
the entire input sequence,
any single vector.

42
00:01:38.200 --> 00:01:40.790
The models developed
in this paper,

43
00:01:40.790 --> 00:01:43.040
RNN search 13-15,

44
00:01:43.040 --> 00:01:45.020
use bidirectional encoders and

45
00:01:45.020 --> 00:01:47.180
decoders, but with attention.

46
00:01:47.180 --> 00:01:50.150
First, these models
perform better than

47
00:01:50.150 --> 00:01:51.380
the traditional seek to seek

48
00:01:51.380 --> 00:01:53.690
models across all
sentence length.

49
00:01:53.690 --> 00:01:56.360
The RNN search 50 model has

50
00:01:56.360 --> 00:01:57.680
basically no fall off in

51
00:01:57.680 --> 00:02:00.320
performance as sentence
lengths increase.

52
00:02:00.320 --> 00:02:02.450
As you will see, this is because

53
00:02:02.450 --> 00:02:04.220
the models are able to focus on

54
00:02:04.220 --> 00:02:06.170
specific inputs to predict

55
00:02:06.170 --> 00:02:08.134
words in the output translation,

56
00:02:08.134 --> 00:02:11.680
instead of having to memorize
the entire input sentence.

57
00:02:11.680 --> 00:02:13.550
Now I'll show you the motivation

58
00:02:13.550 --> 00:02:16.190
behind attention
and how it works.

59
00:02:16.190 --> 00:02:18.755
Traditional seek to seek models,

60
00:02:18.755 --> 00:02:20.330
use the final hidden states of

61
00:02:20.330 --> 00:02:23.960
the encoder as the initial
hidden state of the decoder.

62
00:02:23.960 --> 00:02:26.780
This forces the encoder
to store the meaning of

63
00:02:26.780 --> 00:02:30.995
the entire input sequence
into this one hidden states.

64
00:02:30.995 --> 00:02:34.595
Instead of using only
the final hidden states,

65
00:02:34.595 --> 00:02:37.660
you can pass all the hidden
states to the decoder.

66
00:02:37.660 --> 00:02:40.900
However, this quickly
becomes inefficient as you

67
00:02:40.900 --> 00:02:42.445
must retain the
hidden states for

68
00:02:42.445 --> 00:02:44.230
each input step in memory.

69
00:02:44.230 --> 00:02:46.000
To solve this, you can

70
00:02:46.000 --> 00:02:48.340
combine the hidden
states into one vector,

71
00:02:48.340 --> 00:02:51.265
typically called
the context vector.

72
00:02:51.265 --> 00:02:55.135
The samples operation here
is the point-wise addition.

73
00:02:55.135 --> 00:02:58.030
Since the hidden vectors
are all the same size,

74
00:02:58.030 --> 00:03:00.370
you can just add up
these vector elements by

75
00:03:00.370 --> 00:03:04.495
elements to produce another
vector of the same size.

76
00:03:04.495 --> 00:03:07.180
But now the decoder is

77
00:03:07.180 --> 00:03:09.460
getting information
about each step.

78
00:03:09.460 --> 00:03:12.115
But It really only
needs information

79
00:03:12.115 --> 00:03:13.840
from the first few inputs steps

80
00:03:13.840 --> 00:03:15.485
to predict the first word.

81
00:03:15.485 --> 00:03:17.830
This isn't that much
different from using

82
00:03:17.830 --> 00:03:21.580
the last hidden states
from LSTM or GRU.

83
00:03:21.580 --> 00:03:24.700
The solution here is to wait

84
00:03:24.700 --> 00:03:26.740
certain encoder vectors more

85
00:03:26.740 --> 00:03:28.690
than others before the
point-wise addition,

86
00:03:28.690 --> 00:03:30.460
[inaudible] are
more important for

87
00:03:30.460 --> 00:03:33.505
the next decoder outputs
would have larger weights.

88
00:03:33.505 --> 00:03:35.230
That this way, the
context vector

89
00:03:35.230 --> 00:03:36.580
holds more information about

90
00:03:36.580 --> 00:03:38.200
the most important words and

91
00:03:38.200 --> 00:03:40.970
less information
about other words.

92
00:03:40.980 --> 00:03:44.140
But how are these
weights calculated to

93
00:03:44.140 --> 00:03:47.695
determine which input words
are important at each step?

94
00:03:47.695 --> 00:03:50.290
The decoders previous
hidden states,

95
00:03:50.290 --> 00:03:54.400
denoted as S_I minus 1,

96
00:03:54.400 --> 00:03:56.800
contains information
about the previous words

97
00:03:56.800 --> 00:03:58.584
in the output translation.

98
00:03:58.584 --> 00:04:01.720
This means, you can compare
the decoder states with

99
00:04:01.720 --> 00:04:02.890
each encoder state to

100
00:04:02.890 --> 00:04:05.630
determine the most
important inputs.

101
00:04:05.630 --> 00:04:10.190
Intuitively, the decoder can
set the weights such that if

102
00:04:10.190 --> 00:04:13.160
it focuses on only the
most important inputs

103
00:04:13.160 --> 00:04:14.869
words for the next prediction,

104
00:04:14.869 --> 00:04:16.610
it decides which parts of

105
00:04:16.610 --> 00:04:18.905
the input sequence
to pay attention to.

106
00:04:18.905 --> 00:04:22.100
Now step into the
attention layer to examine

107
00:04:22.100 --> 00:04:25.955
how the weights and context
vector are calculated.

108
00:04:25.955 --> 00:04:29.570
The goal of the attention
layer is to return

109
00:04:29.570 --> 00:04:31.910
a context vector that contains

110
00:04:31.910 --> 00:04:35.030
the relevant information
from the encoder states.

111
00:04:35.030 --> 00:04:38.945
The first step is to
calculate the alignments,

112
00:04:38.945 --> 00:04:43.190
E_IJ, which is a
score of how well

113
00:04:43.190 --> 00:04:47.500
the inputs around J match
the expected output its I.

114
00:04:47.500 --> 00:04:49.500
The more the much, the higher

115
00:04:49.500 --> 00:04:51.570
of his score we will expect.

116
00:04:51.570 --> 00:04:54.620
This is done using the
feedforward neural network

117
00:04:54.620 --> 00:04:57.695
with the encoder and decoder
hidden states as inputs,

118
00:04:57.695 --> 00:05:00.130
where the weights for the
feedforward network are

119
00:05:00.130 --> 00:05:03.380
learned along with the rest
of the seek to seek model.

120
00:05:03.380 --> 00:05:06.845
The scores are then
turned into weights

121
00:05:06.845 --> 00:05:10.490
which range from zero to one
using the softmax function.

122
00:05:10.490 --> 00:05:13.190
This means the weights
can be thought of as

123
00:05:13.190 --> 00:05:17.045
a probability distribution
which sum to one.

124
00:05:17.045 --> 00:05:20.810
Finally, each encoder
states is multiplied by

125
00:05:20.810 --> 00:05:22.340
its respective weights and sum

126
00:05:22.340 --> 00:05:25.070
together into one
context vector.

127
00:05:25.070 --> 00:05:28.070
Since the weights are the
probability distribution,

128
00:05:28.070 --> 00:05:30.020
this is equivalent
to calculating

129
00:05:30.020 --> 00:05:33.155
an expected value
across word alignments.

130
00:05:33.155 --> 00:05:34.820
Next up, you'll get

131
00:05:34.820 --> 00:05:37.790
a better understanding
of how all this works by

132
00:05:37.790 --> 00:05:40.040
implementing a simple version of

133
00:05:40.040 --> 00:05:42.865
the attention operation
from the Bahdanau paper.

134
00:05:42.865 --> 00:05:44.990
I have now shown
you how attention

135
00:05:44.990 --> 00:05:47.045
works and why it is important.

136
00:05:47.045 --> 00:05:48.530
In the next video,

137
00:05:48.530 --> 00:05:50.360
I will define what our keys,

138
00:05:50.360 --> 00:05:52.250
queries and values, and

139
00:05:52.250 --> 00:05:55.200
show you how to use
them in attention.