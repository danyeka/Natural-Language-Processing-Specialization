WEBVTT

1
00:00:00.000 --> 00:00:03.630
I have been talking about
the CBOW model for a while.

2
00:00:03.630 --> 00:00:06.420
Now, is finally the
time to actually show

3
00:00:06.420 --> 00:00:09.570
you the architecture that is
used to train this model.

4
00:00:09.570 --> 00:00:12.160
Without further
ado, let's dive in.

5
00:00:12.160 --> 00:00:14.910
The continuous bag of
words model is based on

6
00:00:14.910 --> 00:00:18.270
a shallow dense neural
network with an input layer,

7
00:00:18.270 --> 00:00:19.845
a single hidden layer,

8
00:00:19.845 --> 00:00:21.375
and an output layer.

9
00:00:21.375 --> 00:00:23.370
As you remember, the inputs of

10
00:00:23.370 --> 00:00:25.830
the model is a vector
of context words,

11
00:00:25.830 --> 00:00:27.420
which I'll call x,

12
00:00:27.420 --> 00:00:28.500
and the output is

13
00:00:28.500 --> 00:00:30.570
the vector of the
predicted center word,

14
00:00:30.570 --> 00:00:32.460
which I'll call y hat.

15
00:00:32.460 --> 00:00:34.140
The size of these vectors is

16
00:00:34.140 --> 00:00:36.210
equal to the size
of the vocabulary,

17
00:00:36.210 --> 00:00:39.405
which I'll call V.
So the inputs and

18
00:00:39.405 --> 00:00:41.340
output size of this
neural network is

19
00:00:41.340 --> 00:00:43.684
equal to V. For instance,

20
00:00:43.684 --> 00:00:44.885
if the corpus is,

21
00:00:44.885 --> 00:00:46.685
I am happy because I'm learning,

22
00:00:46.685 --> 00:00:48.950
you have a five-word vocabulary.

23
00:00:48.950 --> 00:00:51.695
So V equals 5 and you'll create

24
00:00:51.695 --> 00:00:53.585
a neural network
where the input and

25
00:00:53.585 --> 00:00:56.260
output layers each
have five neurons.

26
00:00:56.260 --> 00:00:57.494
In the real-world.

27
00:00:57.494 --> 00:00:59.030
corporal, this number

28
00:00:59.030 --> 00:01:01.040
will typically be
in the thousands.

29
00:01:01.040 --> 00:01:02.870
Next is the hidden layer.

30
00:01:02.870 --> 00:01:04.445
If you remember from the lesson

31
00:01:04.445 --> 00:01:06.245
on the word embedding process,

32
00:01:06.245 --> 00:01:08.750
the size or dimension of
the word embeddings is

33
00:01:08.750 --> 00:01:12.110
a hyperparameter of the model
which you select yourself.

34
00:01:12.110 --> 00:01:14.970
Let's call the size of the
word embedding capital N.

35
00:01:14.970 --> 00:01:18.790
Where capital N can
typically be 100-1,000.

36
00:01:18.790 --> 00:01:21.815
You will choose the size
of the hidden layer to

37
00:01:21.815 --> 00:01:24.505
equal the size of the
expected word embedding.

38
00:01:24.505 --> 00:01:28.160
If you choose the size of
the word embeddings to be N,

39
00:01:28.160 --> 00:01:30.020
then you will choose
the hidden layer for

40
00:01:30.020 --> 00:01:32.495
this network to have N neurons,

41
00:01:32.495 --> 00:01:35.015
I'll call the vector
representing the hidden layer

42
00:01:35.015 --> 00:01:38.255
h. This is irregular
feed-forward network,

43
00:01:38.255 --> 00:01:40.765
also called a dense
neural network,

44
00:01:40.765 --> 00:01:43.325
so the three layers
are fully connected.

45
00:01:43.325 --> 00:01:45.170
I'll call the weights
matrix between

46
00:01:45.170 --> 00:01:47.735
the input layer and
hidden layer W_1,

47
00:01:47.735 --> 00:01:50.560
and the bias vector of
the hidden layer is b_1.

48
00:01:50.560 --> 00:01:54.290
Similarly, W_2 is the
weighting matrix between

49
00:01:54.290 --> 00:01:57.110
the hidden layer and
the output layer and

50
00:01:57.110 --> 00:01:59.995
b_2 is the bias vector
of the output layer.

51
00:01:59.995 --> 00:02:01.940
These are the matrices
and vectors that

52
00:02:01.940 --> 00:02:04.685
the neural network will be
learning as you train it.

53
00:02:04.685 --> 00:02:07.415
To give you a little preview
of where you are going,

54
00:02:07.415 --> 00:02:09.470
you will derive the
word embeddings from

55
00:02:09.470 --> 00:02:11.600
the weights matrices of
this neural network.

56
00:02:11.600 --> 00:02:13.475
You will see more
about this later.

57
00:02:13.475 --> 00:02:15.380
You now need
activation functions

58
00:02:15.380 --> 00:02:17.525
for the hidden layer
and output layer.

59
00:02:17.525 --> 00:02:19.220
The results of the activation of

60
00:02:19.220 --> 00:02:21.860
the hidden layer is what
gets sent to the next layer,

61
00:02:21.860 --> 00:02:24.260
which in this case
is the output layer.

62
00:02:24.260 --> 00:02:27.350
Similarly, the outcome
of the activation of

63
00:02:27.350 --> 00:02:29.090
the output layer is what

64
00:02:29.090 --> 00:02:31.195
will be shown as the
model's prediction.

65
00:02:31.195 --> 00:02:32.420
For the hidden layer,

66
00:02:32.420 --> 00:02:36.140
you will use the rectified
linear units function or ReLu.

67
00:02:36.140 --> 00:02:37.460
For the output layer,

68
00:02:37.460 --> 00:02:39.220
you will use the
softmax function.

69
00:02:39.220 --> 00:02:41.060
The activation
functions themselves

70
00:02:41.060 --> 00:02:42.560
deserve a proper discussion,

71
00:02:42.560 --> 00:02:45.385
so I'll describe them in
one of the next videos.

72
00:02:45.385 --> 00:02:47.749
There you have it,
the architecture

73
00:02:47.749 --> 00:02:50.030
of the continuous
bag of words model.

74
00:02:50.030 --> 00:02:51.950
Next up, I'll go through

75
00:02:51.950 --> 00:02:54.940
the dimensions of the matrices
and vectors I mentioned.

76
00:02:54.940 --> 00:02:57.140
Another important
component you'll need for

77
00:02:57.140 --> 00:02:59.600
completing this week's
fun assignments.

78
00:02:59.600 --> 00:03:01.670
In this video, you have seen

79
00:03:01.670 --> 00:03:04.060
the overall structure
of your algorithm.

80
00:03:04.060 --> 00:03:06.215
You're trying to
predict the middle word

81
00:03:06.215 --> 00:03:08.975
given the words outside
or the context words.

82
00:03:08.975 --> 00:03:12.740
However, you have V possible
ways that you can predict.

83
00:03:12.740 --> 00:03:15.350
You cannot use a logistic
regression in this case,

84
00:03:15.350 --> 00:03:17.540
because the logistic
regression outputs

85
00:03:17.540 --> 00:03:19.770
two possibilities.