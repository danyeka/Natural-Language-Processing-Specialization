WEBVTT

1
00:00:00.000 --> 00:00:02.280
This week, I'll be talking about

2
00:00:02.280 --> 00:00:04.770
transfer learning with
the full transformer.

3
00:00:04.770 --> 00:00:07.320
I'll also talk a
little bit about BERT,

4
00:00:07.320 --> 00:00:10.230
which is the Bidirectional
Encoder Representation

5
00:00:10.230 --> 00:00:11.490
for Transformers.

6
00:00:11.490 --> 00:00:13.140
Then I'll talk about

7
00:00:13.140 --> 00:00:15.525
a special model known
as the T5 model,

8
00:00:15.525 --> 00:00:16.950
what you learned about.

9
00:00:16.950 --> 00:00:20.175
Now all of these concepts make
use of transfer learning.

10
00:00:20.175 --> 00:00:23.325
What is transfer learning?
Let me show you.

11
00:00:23.325 --> 00:00:26.490
You'll now take a look at the
transfer learning options

12
00:00:26.490 --> 00:00:30.075
that you will have while
performing your NLP tasks.

13
00:00:30.075 --> 00:00:34.385
This is a quick recap where
you have your train data,

14
00:00:34.385 --> 00:00:35.720
goes into a model,

15
00:00:35.720 --> 00:00:37.420
then you have a prediction.

16
00:00:37.420 --> 00:00:41.450
Transfer learning will
come in two basic forms.

17
00:00:41.450 --> 00:00:45.035
The first one is using
feature-based learning,

18
00:00:45.035 --> 00:00:47.420
and the other one is
using fine-tuning.

19
00:00:47.420 --> 00:00:49.640
By feature-based, I mean

20
00:00:49.640 --> 00:00:52.685
things like word
vectors being learnt.

21
00:00:52.685 --> 00:00:54.860
Fine-tuning is you take

22
00:00:54.860 --> 00:00:56.705
an existing model,
existing weights,

23
00:00:56.705 --> 00:00:58.970
and then you tweak them a
little bit to make sure that

24
00:00:58.970 --> 00:01:01.760
they work on the specific
task you're working on.

25
00:01:01.760 --> 00:01:04.820
There is pre-trained
data which makes

26
00:01:04.820 --> 00:01:07.520
use of labeled and
unlabeled data,

27
00:01:07.520 --> 00:01:09.289
let's say you're training

28
00:01:09.289 --> 00:01:13.090
your model on sentiments
for product reviews.

29
00:01:13.090 --> 00:01:15.380
Then you can use those
same weights to train

30
00:01:15.380 --> 00:01:18.415
your model on course reviews.

31
00:01:18.415 --> 00:01:21.710
Then the other thing
is pre-training task,

32
00:01:21.710 --> 00:01:25.010
which usually makes use
of language modeling.

33
00:01:25.010 --> 00:01:26.615
For example, you mask

34
00:01:26.615 --> 00:01:29.225
a word and you try to
predict what that word is,

35
00:01:29.225 --> 00:01:33.115
or you try to predict what
the next sentence would be.

36
00:01:33.115 --> 00:01:35.705
Let's look at
general-purpose learning,

37
00:01:35.705 --> 00:01:38.390
and this is something that
you're already familiar with.

38
00:01:38.390 --> 00:01:41.060
Over here you have
I am because I'm

39
00:01:41.060 --> 00:01:43.925
learning and you're trying
to predict the central word.

40
00:01:43.925 --> 00:01:46.810
The central word is
happy over here.

41
00:01:46.810 --> 00:01:49.460
You used a model known

42
00:01:49.460 --> 00:01:51.665
as the continuous
bag-of-words model.

43
00:01:51.665 --> 00:01:54.505
Or at least you got the
following embeddings.

44
00:01:54.505 --> 00:01:58.045
Now you can use
these embeddings as

45
00:01:58.045 --> 00:01:59.680
the inputs features and

46
00:01:59.680 --> 00:02:03.785
translation tasks to translate
from English to German.

47
00:02:03.785 --> 00:02:07.375
Let's look at a more
concrete example

48
00:02:07.375 --> 00:02:10.195
of feature-based
versus fine-tuning.

49
00:02:10.195 --> 00:02:12.265
Where you have word embeddings,

50
00:02:12.265 --> 00:02:13.990
put it in your model,

51
00:02:13.990 --> 00:02:15.580
you get a prediction.

52
00:02:15.580 --> 00:02:18.100
Then over here you get

53
00:02:18.100 --> 00:02:21.700
some type of new features
or new word embeddings.

54
00:02:21.700 --> 00:02:25.630
Then you feed them into a
completely different model.

55
00:02:25.630 --> 00:02:27.815
This gives you your prediction.

56
00:02:27.815 --> 00:02:30.190
Now, on the fine-tuning side,

57
00:02:30.190 --> 00:02:31.210
you have your embeddings,

58
00:02:31.210 --> 00:02:32.470
you feed it into your model,

59
00:02:32.470 --> 00:02:34.510
you get a prediction, and

60
00:02:34.510 --> 00:02:36.880
then you fine-tune
on this model,

61
00:02:36.880 --> 00:02:39.130
on the downstream task.

62
00:02:39.130 --> 00:02:42.990
Then you have some new inputs
on your new weight so that

63
00:02:42.990 --> 00:02:47.110
you fine-tune so you feed it
in and you get a prediction.

64
00:02:47.110 --> 00:02:49.110
Let's look at fine-tuning.

65
00:02:49.110 --> 00:02:52.550
This is a way to add
fine-tuning to your model.

66
00:02:52.550 --> 00:02:55.550
Let's say you have movies and
you're predicting one star,

67
00:02:55.550 --> 00:02:57.475
two stars, or three stars.

68
00:02:57.475 --> 00:02:59.540
You pre-trained and let's

69
00:02:59.540 --> 00:03:01.145
say now you have course reviews.

70
00:03:01.145 --> 00:03:03.680
One way you can do this is you

71
00:03:03.680 --> 00:03:07.630
fix all of these weights
that you already have.

72
00:03:07.630 --> 00:03:10.055
Then you add a new
feed-forward network

73
00:03:10.055 --> 00:03:12.515
while keeping everything
else frozen here.

74
00:03:12.515 --> 00:03:13.880
Then you just tune on

75
00:03:13.880 --> 00:03:16.505
this new network
that you just added.

76
00:03:16.505 --> 00:03:18.900
Data affects performance a lot.

77
00:03:18.900 --> 00:03:21.530
In this case, you have
data being fed into

78
00:03:21.530 --> 00:03:25.370
your model and you get
some neutral outcome.

79
00:03:25.370 --> 00:03:27.575
But as you have more data

80
00:03:27.575 --> 00:03:30.500
and you build a larger
model over here,

81
00:03:30.500 --> 00:03:32.840
then you get a much
better outcome.

82
00:03:32.840 --> 00:03:34.370
The more data you have,

83
00:03:34.370 --> 00:03:36.770
the better it is, and
the more data you have,

84
00:03:36.770 --> 00:03:39.740
the bigger the models
you can build will

85
00:03:39.740 --> 00:03:43.375
be able to capture the task
you're trying to predict on.

86
00:03:43.375 --> 00:03:47.015
Let's look at labeled
versus unlabeled data.

87
00:03:47.015 --> 00:03:49.730
Now, this is just
a graphic example

88
00:03:49.730 --> 00:03:51.440
that tells you that usually

89
00:03:51.440 --> 00:03:54.365
you'll have way more
unlabeled text data

90
00:03:54.365 --> 00:03:56.205
than labeled text data.

91
00:03:56.205 --> 00:03:58.245
Here's an example.

92
00:03:58.245 --> 00:04:00.840
In pre-training you
have no labels.

93
00:04:00.840 --> 00:04:03.120
You feed this into your model.

94
00:04:03.120 --> 00:04:05.420
Then in downstream task,

95
00:04:05.420 --> 00:04:07.805
you could have something like,

96
00:04:07.805 --> 00:04:09.785
what day is Pi day,

97
00:04:09.785 --> 00:04:14.050
feed this into your model
and you get the March 14th.

98
00:04:14.050 --> 00:04:15.640
Remember, these are the word

99
00:04:15.640 --> 00:04:17.510
vectors we're talking about.

100
00:04:17.510 --> 00:04:20.395
Which tasks work
with unlabeled data?

101
00:04:20.395 --> 00:04:22.480
This is a self-supervised task,

102
00:04:22.480 --> 00:04:24.275
so you have the unlabeled data,

103
00:04:24.275 --> 00:04:26.360
and then you create
input features.

104
00:04:26.360 --> 00:04:28.400
You create targets or labels.

105
00:04:28.400 --> 00:04:30.005
This is how it works.

106
00:04:30.005 --> 00:04:31.400
You have unlabeled data,

107
00:04:31.400 --> 00:04:33.170
learning from
deep-learning AI is

108
00:04:33.170 --> 00:04:35.620
like watching the sunset
with my best friend.

109
00:04:35.620 --> 00:04:39.180
You create the inputs
and then blank.

110
00:04:39.180 --> 00:04:41.700
The sunset with my best blank.

111
00:04:41.700 --> 00:04:43.395
You're trying to predict friend.

112
00:04:43.395 --> 00:04:45.165
You feed this into your model.

113
00:04:45.165 --> 00:04:46.890
You get your prediction.

114
00:04:46.890 --> 00:04:48.525
The target is friend,

115
00:04:48.525 --> 00:04:50.205
here prediction goes in.

116
00:04:50.205 --> 00:04:52.450
You have the loss
here and then you

117
00:04:52.450 --> 00:04:54.805
use this loss to
update your model.

118
00:04:54.805 --> 00:04:57.010
This is basic language modeling

119
00:04:57.010 --> 00:04:58.810
that you've already seen before.

120
00:04:58.810 --> 00:05:02.830
Let's look at fine-tuning a
model in the downstream task.

121
00:05:02.830 --> 00:05:04.440
You have your model here.

122
00:05:04.440 --> 00:05:07.105
You did some pre-training
on it either by

123
00:05:07.105 --> 00:05:10.660
masking words or predicting
the next sentence.

124
00:05:10.660 --> 00:05:13.980
Then use this model to train
it on downstream tasks.

125
00:05:13.980 --> 00:05:15.640
You can use this model,

126
00:05:15.640 --> 00:05:18.305
fine-tune it on translation

127
00:05:18.305 --> 00:05:22.350
or summarization, or
question answering.

128
00:05:22.350 --> 00:05:25.030
Here's a summary of what
you've seen so far.

129
00:05:25.030 --> 00:05:27.140
Usually you have
some train data,

130
00:05:27.140 --> 00:05:29.530
you have the model,
you make a prediction.

131
00:05:29.530 --> 00:05:31.970
We use transfer learning to get

132
00:05:31.970 --> 00:05:34.595
feature-based examples
or fine-tuning.

133
00:05:34.595 --> 00:05:38.060
Feature-based like word
vectors or word embeddings and

134
00:05:38.060 --> 00:05:40.265
fine-tuning is something that's

135
00:05:40.265 --> 00:05:42.445
you can do on downstream tasks.

136
00:05:42.445 --> 00:05:44.105
Then we can also use

137
00:05:44.105 --> 00:05:46.850
labeled data and unlabeled
data to help us.

138
00:05:46.850 --> 00:05:48.845
You can train something on

139
00:05:48.845 --> 00:05:52.040
a different task on a
different data set.

140
00:05:52.040 --> 00:05:54.550
and that will also
help a little bit.

141
00:05:54.550 --> 00:05:57.320
Then you can use
pre-training task like

142
00:05:57.320 --> 00:05:59.900
language modeling
or masked words

143
00:05:59.900 --> 00:06:01.655
or next sentence prediction.

144
00:06:01.655 --> 00:06:06.120
You have now seen some
advantages of transfer learning.