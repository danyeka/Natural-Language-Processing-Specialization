WEBVTT

1
00:00:00.440 --> 00:00:04.893
Queries, keys and values are terms
that you will be using for

2
00:00:04.893 --> 00:00:06.827
attention in this video.

3
00:00:06.827 --> 00:00:09.968
I will define them for you and
show you how they could be used.

4
00:00:09.968 --> 00:00:10.961
Let's get started.

5
00:00:12.340 --> 00:00:16.540
The original attention paper
was published in 2014.

6
00:00:16.540 --> 00:00:20.388
Since then there have been multiple
variations on attention with some

7
00:00:20.388 --> 00:00:23.461
models that don't rely on
recurrent neural networks.

8
00:00:24.740 --> 00:00:25.560
For example,

9
00:00:25.560 --> 00:00:30.346
the 2017 paper attention is all you need
to introduce the transformer model and

10
00:00:30.346 --> 00:00:36.040
the form of attention based on information
retrieval, using queries, keys and values.

11
00:00:36.040 --> 00:00:40.322
This is an efficient and powerful form
of attention that you'll be using

12
00:00:40.322 --> 00:00:42.870
in this week's assignment in this video.

13
00:00:42.870 --> 00:00:47.129
I'll show you how this type of attention
works as well as the concept of alignments

14
00:00:47.129 --> 00:00:48.251
between languages.

15
00:00:49.640 --> 00:00:54.340
Conceptually, you can think of keys and
values as a look up table.

16
00:00:54.340 --> 00:00:56.324
The query is matched to a key and

17
00:00:56.324 --> 00:00:59.461
the value associated with
that key is returned.

18
00:01:01.340 --> 00:01:04.775
For example,
if we are translating between french and

19
00:01:04.775 --> 00:01:07.340
english heure matches with time.

20
00:01:07.340 --> 00:01:13.097
So we'd like to get the value for
time, in practice to the queries,

21
00:01:13.097 --> 00:01:17.340
keys and
values are all represented by vectors.

22
00:01:17.340 --> 00:01:19.540
Embedding vectors for example.

23
00:01:19.540 --> 00:01:23.638
Due to this, you don't get exact matches
but the model can learn which words

24
00:01:23.638 --> 00:01:27.840
are the most similar between
the source and target languages.

25
00:01:27.840 --> 00:01:30.840
The similarity between
words is called alignment.

26
00:01:30.840 --> 00:01:34.731
The query and key vectors are used
to calculate alignment scores that

27
00:01:34.731 --> 00:01:38.140
are measures of how well the query and
keys match.

28
00:01:38.140 --> 00:01:42.130
These alignment scores are then
turned into weights used for awaited,

29
00:01:42.130 --> 00:01:43.592
sum of the value vectors,

30
00:01:43.592 --> 00:01:47.861
this weighted sum of the value vectors
is returned as the attention vector.

31
00:01:49.840 --> 00:01:53.640
This process can be performed
using scale dot-product attention.

32
00:01:53.640 --> 00:01:57.990
The queries for each step are packed
together into a matrix Q.

33
00:01:57.990 --> 00:02:01.940
So attention can be computed
simultaneously for each query.

34
00:02:01.940 --> 00:02:05.840
The keys and values are also
packed into matrices K and V.

35
00:02:05.840 --> 00:02:10.301
These matrices are the inputs for the
attention function shown as a diagram on

36
00:02:10.301 --> 00:02:12.851
the left and mathematically on the rights.

37
00:02:14.140 --> 00:02:15.782
First, the queries and

38
00:02:15.782 --> 00:02:21.127
keys matrices are multiplied together
to get a matrix of alignments course.

39
00:02:21.127 --> 00:02:25.753
These are then scaled by the square
root of the key vector dimension,

40
00:02:25.753 --> 00:02:29.104
dk the scaling improves
the model performance for

41
00:02:29.104 --> 00:02:33.751
larger model sizes and could be
seen as a regularization constants.

42
00:02:35.540 --> 00:02:40.440
Next the scale scores are converted to
weights using the softmax function.

43
00:02:40.440 --> 00:02:43.440
Such that the weights for
each query sum to one.

44
00:02:43.440 --> 00:02:47.936
Finally the weights and the value matrices
are multiplied to get the attention

45
00:02:47.936 --> 00:02:53.140
vectors for each query, you can think of
the keys and the values as being the same.

46
00:02:53.140 --> 00:02:57.546
So when you multiply the softmax
output with V you are taking a linear

47
00:02:57.546 --> 00:03:03.040
combination of your initial input which
is then being fed to the decoder.

48
00:03:03.040 --> 00:03:06.940
Take a minute to make sure
what I just said makes sense.

49
00:03:06.940 --> 00:03:12.298
No, that unlike the original form of
attention, scale dot-product attention

50
00:03:12.298 --> 00:03:17.340
consists of only two Matrix
multiplications and no neural networks.

51
00:03:17.340 --> 00:03:21.626
Since matrix multiplication is highly
optimized in modern deep learning

52
00:03:21.626 --> 00:03:22.630
frameworks.

53
00:03:22.630 --> 00:03:25.845
This form of attention is
much faster to compute but

54
00:03:25.845 --> 00:03:29.510
this also means that the alignments
between the source and

55
00:03:29.510 --> 00:03:33.040
target languages must
be learned elsewhere.

56
00:03:33.040 --> 00:03:36.266
Typically, alignment is learned
in the input embeddings or

57
00:03:36.266 --> 00:03:39.540
in other linear layers
before the attention layer.

58
00:03:39.540 --> 00:03:43.740
Before moving on,
I want to look a bit closer at alignment.

59
00:03:43.740 --> 00:03:48.616
The alignment weights form a matrix with
queries, targets words on the roads and

60
00:03:48.616 --> 00:03:50.851
keys or source words on the columns.

61
00:03:51.940 --> 00:03:56.174
Each entry in this matrix is
the weight for the correspondent query,

62
00:03:56.174 --> 00:04:00.554
key pair word pairs that have similar
meanings, K and T, for example,

63
00:04:00.554 --> 00:04:05.440
will have larger weights than
the similar words like day and time.

64
00:04:05.440 --> 00:04:09.924
Through training, the model learns
which words have similar meanings and

65
00:04:09.924 --> 00:04:13.352
encodes that information and
the query and key vectors.

66
00:04:13.352 --> 00:04:16.112
Learning alignment like
this is beneficial for

67
00:04:16.112 --> 00:04:20.940
translating between languages with
different grammatical structures.

68
00:04:20.940 --> 00:04:25.451
Since attention looks at the entire
input and target sentences at once and

69
00:04:25.451 --> 00:04:28.216
calculates alignments based on word pairs,

70
00:04:28.216 --> 00:04:32.540
weights are assigned appropriately
regardless of word order.

71
00:04:32.540 --> 00:04:37.932
For example, In the sentence, the
agreement on the European Economic Area

72
00:04:37.932 --> 00:04:44.010
was signed in August 1992 and this other
sentence lack of lasagne economic open.

73
00:04:44.010 --> 00:04:48.021
I mean you're not meeting of sangatte
revenues, you can see that zone in

74
00:04:48.021 --> 00:04:52.540
the area are at different positions,
let's have the same meaning.

75
00:04:52.540 --> 00:04:56.426
The model has learned to align them
appropriately, allowing the decoder to

76
00:04:56.426 --> 00:04:59.961
focus on the appropriate inputs
words despite different ordering.

77
00:05:01.140 --> 00:05:03.620
Congrats on absorbing
all these new concepts.

78
00:05:03.620 --> 00:05:06.740
I introduced you to the purpose
of an attention layer.

79
00:05:06.740 --> 00:05:10.513
You saw how it is related with
information retrieval and

80
00:05:10.513 --> 00:05:16.640
I showed you how well it works even for
languages with very different structures.

81
00:05:16.640 --> 00:05:20.641
In the next video, I'll be talking
about neural machine translation and

82
00:05:20.641 --> 00:05:23.940
show you what the setup looks like for
the system.

83
00:05:23.940 --> 00:05:27.535
I'll show you what the data set looks
like and the steps required for

84
00:05:27.535 --> 00:05:29.840
pre processing your data sets.

85
00:05:29.840 --> 00:05:33.140
You have now seen what key square ease and
values are.

86
00:05:33.140 --> 00:05:35.911
These are important because if
you read a research paper you

87
00:05:35.911 --> 00:05:39.140
might come across these terms and
you will understand them.

88
00:05:39.140 --> 00:05:40.540
In the next video.

89
00:05:40.540 --> 00:05:43.251
I will talk about the setup for
machine translation.