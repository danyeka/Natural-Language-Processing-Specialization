WEBVTT

1
00:00:00.340 --> 00:00:01.940
Good to see you again.

2
00:00:01.940 --> 00:00:07.166
I'm about to explain arguably the two most
important activation functions used in AI.

3
00:00:07.166 --> 00:00:12.340
The rectified linear unit also known as
the railways and the softmax function.

4
00:00:12.340 --> 00:00:13.790
So let's get started.

5
00:00:13.790 --> 00:00:16.630
Before jumping into the softmax function.

6
00:00:16.630 --> 00:00:21.829
I want to talk a little bit about the ReLU
function or rectified linear units.

7
00:00:21.829 --> 00:00:24.740
Apart from the input
neurons which are used as

8
00:00:24.740 --> 00:00:28.878
is each layer first calculates
the weighted sum of its inputs and

9
00:00:28.878 --> 00:00:33.740
adds a bias, then passes the results
into an activation function.

10
00:00:33.740 --> 00:00:40.730
So for the hidden layer you first
calculate Z1 equals W1 times X + B1.

11
00:00:40.730 --> 00:00:45.659
And to get H you pass Z1 into an
activation function which is the rectified

12
00:00:45.659 --> 00:00:48.537
linear unit function ReLU for this layer.

13
00:00:48.537 --> 00:00:53.470
ReLU is a popular general purpose
activation function that only activates

14
00:00:53.470 --> 00:00:57.308
the neuron every some of its
weighted input is positive and

15
00:00:57.308 --> 00:00:59.470
passes through this result.

16
00:00:59.470 --> 00:01:03.863
If that's the case
mathematically ReLU of X where X

17
00:01:03.863 --> 00:01:08.540
is a real number is equal
to the maximum of 0 &amp; X.

18
00:01:08.540 --> 00:01:10.840
You can see the graph for
this function here.

19
00:01:10.840 --> 00:01:15.480
Here's an example for vectors Z1 to
get vector h representing the value of

20
00:01:15.480 --> 00:01:18.840
the hidden layer you
apply the ReLU function.

21
00:01:18.840 --> 00:01:23.451
The rows from Z1 that's our
negative like negative 0.3 and

22
00:01:23.451 --> 00:01:26.440
negative 4.6 become zeros in h.

23
00:01:26.440 --> 00:01:29.509
And the positive values stay as is so for

24
00:01:29.509 --> 00:01:34.740
instance 5.1 stays as 5.1 and
0.2 is still 0.2.

25
00:01:34.740 --> 00:01:38.487
Now, for the output layer you calculate
the weighted sum of the values from

26
00:01:38.487 --> 00:01:39.940
the hidden layer.

27
00:01:39.940 --> 00:01:44.955
And add by it's vector that's W2h
+ b2 which I'll call Z instead of

28
00:01:44.955 --> 00:01:50.540
Z2 in the previous slides for
reasons that will soon become apparent.

29
00:01:50.540 --> 00:01:54.898
You then passes Z into the activation
function which is the softmax function for

30
00:01:54.898 --> 00:01:55.860
this layer.

31
00:01:55.860 --> 00:01:57.968
To get the output vector y hat.

32
00:01:57.968 --> 00:02:00.940
The softmax function works as follows.

33
00:02:00.940 --> 00:02:05.340
Its input is a vector of real numbers
as opposed to ReLU which in its

34
00:02:05.340 --> 00:02:09.539
original form is a function
of a single real number.

35
00:02:09.539 --> 00:02:15.254
It outputs a vector of real numbers in
the interval 0-1 which some up to one and

36
00:02:15.254 --> 00:02:19.730
can be interpreted as
probabilities of exclusive events.

37
00:02:19.730 --> 00:02:22.180
In the case of the continuous
bag of words model.

38
00:02:22.180 --> 00:02:27.195
When you apply softmax to Z, you obtain
an output vector Y hat with V rows

39
00:02:27.195 --> 00:02:32.540
where each row corresponds to a word
of the vocabulary of the corpus.

40
00:02:32.540 --> 00:02:36.042
The values of the vector can be
interpreted as the probability

41
00:02:36.042 --> 00:02:40.618
that the central word, which is what the
models trying to predict is the word that

42
00:02:40.618 --> 00:02:43.040
is assigned to each of the rows.

43
00:02:43.040 --> 00:02:47.403
Mathematically if the input
to softmax is vector Zi

44
00:02:47.403 --> 00:02:51.540
with element Ci with i
ranging from one to V.

45
00:02:51.540 --> 00:02:57.541
And the resulting vector is Y
hat with elements Y hats of

46
00:02:57.541 --> 00:03:05.340
i then Y hat i equals E to the Zi divided
by the sum over J of E to the ZJ.

47
00:03:05.340 --> 00:03:09.565
The exponential transforms all our
inputs to positive numbers and

48
00:03:09.565 --> 00:03:14.540
the divisor normalizes the vector so
that their roles some up to one.

49
00:03:14.540 --> 00:03:19.554
To illustrate softmax with a numerical
example, let's say that Z is equal

50
00:03:19.554 --> 00:03:24.809
to this vector taking the exponential of
these values gives you this vector and

51
00:03:24.809 --> 00:03:31.140
the rows some up to 97899 which will be
the denominator in the softmax function.

52
00:03:31.140 --> 00:03:35.545
You can now get the values of the we had
vector by dividing the previous vector by

53
00:03:35.545 --> 00:03:37.440
the sum of its elements.

54
00:03:37.440 --> 00:03:43.018
So the first element will be 8103 divided

55
00:03:43.018 --> 00:03:47.870
by 97899 which is 0.083.

56
00:03:47.870 --> 00:03:52.640
And similarly for the other rows and
the sum is of course equal to one.

57
00:03:52.640 --> 00:03:57.679
So mapping the rolls to the vocabulary,
you would interpret this output

58
00:03:57.679 --> 00:04:02.940
vector as meaning that given a specific
input context words vector X.

59
00:04:02.940 --> 00:04:08.353
The probability that the central
world is M 0.083 for

60
00:04:08.353 --> 00:04:13.060
because the probability is 0.03 and so on.

61
00:04:13.060 --> 00:04:19.040
Now the highest value in the vector is
0.61 for the row corresponding to happy.

62
00:04:19.040 --> 00:04:23.640
So this would mean that the model is
predicting that the central word is happy.

63
00:04:23.640 --> 00:04:25.695
You'll get to implement both revenue and

64
00:04:25.695 --> 00:04:28.740
softmax function in
this week's assignment.

65
00:04:28.740 --> 00:04:32.992
So you have seen the two functions
described the ReLU function changes

66
00:04:32.992 --> 00:04:35.154
all the negative numbers to zero and

67
00:04:35.154 --> 00:04:39.913
the softmax function allows you to get
a vector of probabilities over k classes

68
00:04:39.913 --> 00:04:44.240
that tells you how likely an example
is to belong to each class.

69
00:04:44.240 --> 00:04:47.751
Of course, the probabilities for
each class have to sum up to one.