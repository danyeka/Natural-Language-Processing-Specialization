WEBVTT

1
00:00:00.050 --> 00:00:03.915
N-grams are fundamental
concepts in NLP.

2
00:00:03.915 --> 00:00:05.759
You can use them in
speech recognition,

3
00:00:05.759 --> 00:00:08.955
spelling correction,
augmentative communication,

4
00:00:08.955 --> 00:00:10.230
and many other things.

5
00:00:10.230 --> 00:00:11.805
In this video, you will see

6
00:00:11.805 --> 00:00:13.995
an overview of what
you'll be learning.

7
00:00:13.995 --> 00:00:16.500
Let's take a look. This week,

8
00:00:16.500 --> 00:00:18.690
you'll create an
n-gram language model

9
00:00:18.690 --> 00:00:20.250
from a text corpus.

10
00:00:20.250 --> 00:00:22.800
Recall that a text
corpus is typically

11
00:00:22.800 --> 00:00:25.395
a large database
of text documents,

12
00:00:25.395 --> 00:00:28.310
such as all the pages
on the Wikipedia,

13
00:00:28.310 --> 00:00:32.285
all books from one author or
all tweets from one account.

14
00:00:32.285 --> 00:00:34.400
A language model is a tool that

15
00:00:34.400 --> 00:00:37.070
calculates the
probabilities of sentences.

16
00:00:37.070 --> 00:00:40.280
You can think of a sentence
as a sequence of words.

17
00:00:40.280 --> 00:00:43.040
Language models
can also estimate

18
00:00:43.040 --> 00:00:45.020
the probability of
an upcoming word

19
00:00:45.020 --> 00:00:47.405
given a history of
previous words.

20
00:00:47.405 --> 00:00:51.095
For example, if you
begin an e-mail, "Hello,

21
00:00:51.095 --> 00:00:54.440
how are," your e-mail
application may guess

22
00:00:54.440 --> 00:00:57.830
that the next word you want
to write is the word you,

23
00:00:57.830 --> 00:00:59.980
as in hello, how are you?

24
00:00:59.980 --> 00:01:01.305
In the assignment,

25
00:01:01.305 --> 00:01:04.025
you'll build your own
n-gram language model

26
00:01:04.025 --> 00:01:07.310
and apply it to auto
complete a given sentence.

27
00:01:07.310 --> 00:01:08.990
First it will process

28
00:01:08.990 --> 00:01:11.360
a text corpus into
a language model.

29
00:01:11.360 --> 00:01:13.505
Users of your
autocomplete system

30
00:01:13.505 --> 00:01:16.360
will provide the starting
words of a sentence.

31
00:01:16.360 --> 00:01:18.110
The model that you will build

32
00:01:18.110 --> 00:01:19.730
will then allow you to query

33
00:01:19.730 --> 00:01:21.245
for the most likely words

34
00:01:21.245 --> 00:01:23.440
following the starts
of that sentence.

35
00:01:23.440 --> 00:01:25.160
Then it's outputs that as

36
00:01:25.160 --> 00:01:28.205
a suggestion to
complete the sentence.

37
00:01:28.205 --> 00:01:30.500
While you are going to focus on

38
00:01:30.500 --> 00:01:32.615
the autocomplete
application this week,

39
00:01:32.615 --> 00:01:36.140
language models are widely
used in other areas as well.

40
00:01:36.140 --> 00:01:37.790
Language models are used in

41
00:01:37.790 --> 00:01:39.200
speech recognition to convert

42
00:01:39.200 --> 00:01:41.150
the outputs into real words.

43
00:01:41.150 --> 00:01:43.610
For example, if
an acoustic model

44
00:01:43.610 --> 00:01:46.340
converts speech to
text, and it's hears,

45
00:01:46.340 --> 00:01:50.350
"I saw a van," then the
speech to text system can use

46
00:01:50.350 --> 00:01:52.550
a language model
to know that it's

47
00:01:52.550 --> 00:01:54.800
more likely the sentence was,

48
00:01:54.800 --> 00:01:58.645
"I saw a van" and not
"eyes awe of an."

49
00:01:58.645 --> 00:02:02.780
Language models are also used
in spelling correction to

50
00:02:02.780 --> 00:02:04.760
identify words that should be

51
00:02:04.760 --> 00:02:07.240
replaced based on the
sentence there in.

52
00:02:07.240 --> 00:02:09.470
Probabilities are also important

53
00:02:09.470 --> 00:02:12.115
for augmentative
communication systems.

54
00:02:12.115 --> 00:02:15.200
These are systems that
can take a series of

55
00:02:15.200 --> 00:02:17.015
hand gestures from a user

56
00:02:17.015 --> 00:02:19.535
to help them form
words and sentences.

57
00:02:19.535 --> 00:02:21.260
People like Stephen Hawking,

58
00:02:21.260 --> 00:02:24.065
who are unable to
physically talk or sign,

59
00:02:24.065 --> 00:02:27.410
can instead use simple movements
to select towards from

60
00:02:27.410 --> 00:02:31.100
a menu and then the system
can speak for them.

61
00:02:31.100 --> 00:02:33.920
Word prediction using
language models can

62
00:02:33.920 --> 00:02:37.145
be used to suggest likely
words for the menu.

63
00:02:37.145 --> 00:02:39.415
Here's what you'll
be doing this week.

64
00:02:39.415 --> 00:02:41.330
First, you will transform

65
00:02:41.330 --> 00:02:44.300
your raw text corpus
into a language model,

66
00:02:44.300 --> 00:02:46.010
which returns the probability of

67
00:02:46.010 --> 00:02:49.600
the next word by using the
previous words of a sentence.

68
00:02:49.600 --> 00:02:52.310
Next, you'll adapt
your language model to

69
00:02:52.310 --> 00:02:55.415
deal with words the model
hasn't seen during training.

70
00:02:55.415 --> 00:02:59.140
These words are called
out of vocabulary words.

71
00:02:59.140 --> 00:03:01.820
Smoothing is another
technique that you can

72
00:03:01.820 --> 00:03:04.910
use to deal with
previously unseen inputs.

73
00:03:04.910 --> 00:03:07.190
Probability of unseen words and

74
00:03:07.190 --> 00:03:10.990
sentences can still be
successfully estimated this way.

75
00:03:10.990 --> 00:03:13.790
Smoothing essentially
pretends that each word and

76
00:03:13.790 --> 00:03:17.045
phrase appears in the training
corpus at least once.

77
00:03:17.045 --> 00:03:18.380
This helps to calculate

78
00:03:18.380 --> 00:03:22.265
the probability even for
unusual words and sequences.

79
00:03:22.265 --> 00:03:24.650
Finally, I'll show
you how to choose

80
00:03:24.650 --> 00:03:27.890
the best language model
with the perplexity metric,

81
00:03:27.890 --> 00:03:30.305
a new tool for your toolkits.

82
00:03:30.305 --> 00:03:31.910
When you combine these skills,

83
00:03:31.910 --> 00:03:34.130
you'll be able to
successfully implement

84
00:03:34.130 --> 00:03:36.290
a sentence auto-completion model

85
00:03:36.290 --> 00:03:38.455
in this week's assignments.

86
00:03:38.455 --> 00:03:40.505
Now that you have an overview

87
00:03:40.505 --> 00:03:41.800
of what you're going to learn,

88
00:03:41.800 --> 00:03:43.910
we will dive in and
start by showing

89
00:03:43.910 --> 00:03:47.550
you n-gram probabilities
in the next video.