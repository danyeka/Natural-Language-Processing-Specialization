WEBVTT

1
00:00:00.000 --> 00:00:02.671
When you train an N-gram
on a limited corpus.

2
00:00:02.671 --> 00:00:05.586
The probabilities of
some words may be skewed.

3
00:00:05.586 --> 00:00:10.303
In this video, I'll show you how to remedy
that with a method called smoothing.

4
00:00:10.303 --> 00:00:14.793
First, you will see an example of how
N-grams missing from the corpus affect

5
00:00:14.793 --> 00:00:17.290
the estimation of N-grams probability.

6
00:00:17.290 --> 00:00:20.463
Next, I'll go over some
popular smoothing techniques.

7
00:00:20.463 --> 00:00:26.110
In the last section, I'll touch on other
methods such as backoff and interpolation.

8
00:00:26.110 --> 00:00:29.540
Now that you've resolved the issue
of completely unknown words,

9
00:00:29.540 --> 00:00:32.673
it's time to address another
case of missing information.

10
00:00:32.673 --> 00:00:37.615
For example, how would you manage the
probability of an N-gram made up of words

11
00:00:37.615 --> 00:00:42.068
occurring in the corpus, but
where the N-gram itself is not present?

12
00:00:42.068 --> 00:00:47.063
Remember, you had the corpus of three
sentences earlier made up of N-grams like

13
00:00:47.063 --> 00:00:49.052
eats chocolate, John drinks.

14
00:00:49.052 --> 00:00:53.933
Notice that both of the words John and
eats are present in the corpus, but

15
00:00:53.933 --> 00:00:56.347
the by-gram John eats is missing.

16
00:00:56.347 --> 00:00:59.758
The count of the by-gram
John it's would be 0 and

17
00:00:59.758 --> 00:01:03.343
the probability of the by-gram
would be 0 as well.

18
00:01:03.343 --> 00:01:07.387
Everything that did not occur in
the corpus would be considered impossible.

19
00:01:07.387 --> 00:01:11.957
An estimation of the probability from
counts wouldn't work in this case.

20
00:01:11.957 --> 00:01:17.592
Smoothing is a technique that can help you
deal with the situation and N-gram models.

21
00:01:17.592 --> 00:01:20.867
You might remember smoothing
from the previous week,

22
00:01:20.867 --> 00:01:25.786
where it was used in the transition matrix
and probabilities for parts of speech.

23
00:01:25.786 --> 00:01:29.744
Here, you'll be using this method for
N-gram probabilities.

24
00:01:29.744 --> 00:01:31.539
What does smoothing mean?

25
00:01:31.539 --> 00:01:32.651
Let's focus for

26
00:01:32.651 --> 00:01:37.586
now on add-one smoothing which is
also called the Laplacian smoothing.

27
00:01:37.586 --> 00:01:42.163
Add-one smoothing mathematically
changes the formula for

28
00:01:42.163 --> 00:01:45.129
the N-gram probability of the word n.

29
00:01:45.129 --> 00:01:51.210
Based off its history here, you can see
the by-gram probability of the word wn,

30
00:01:51.210 --> 00:01:53.812
given the previous words wn-1.

31
00:01:53.812 --> 00:01:56.657
But it's used in the same
way to general N-grams.

32
00:01:56.657 --> 00:02:01.147
Add-one smoothing, it just says let's
add 1 both to the numerator and

33
00:02:01.147 --> 00:02:03.730
to each by-gram in the denominator sum.

34
00:02:03.730 --> 00:02:09.433
This change can be interpreted as
add-one occurrence to each by-gram.

35
00:02:09.433 --> 00:02:14.772
So by-grams that are missing in the corpus
would now have a nonzero probability.

36
00:02:14.772 --> 00:02:18.232
In the denominator, you're adding 1 for

37
00:02:18.232 --> 00:02:22.949
each possible by-gram,
starting with the word wn-1.

38
00:02:22.949 --> 00:02:27.905
This corresponds to adding 1 to each
cell in the row indexed by the word

39
00:02:27.905 --> 00:02:29.923
wn-1 in the count matrix.

40
00:02:29.923 --> 00:02:36.437
Then repeat this for as many times as
there are words in the vocabulary.

41
00:02:36.437 --> 00:02:38.873
You can take the 1 out of the sum and

42
00:02:38.873 --> 00:02:42.499
add the size of the vocabulary
to the denominator.

43
00:02:42.499 --> 00:02:47.354
This will only work on a corpus where
the real counts are large enough

44
00:02:47.354 --> 00:02:49.752
to outweigh the plus one though.

45
00:02:49.752 --> 00:02:55.001
Otherwise, the probabilities of missing
words would be too high, but add-one

46
00:02:55.001 --> 00:03:00.500
smoothing helps quite a lot because now
there are no by-grams with 0 probability.

47
00:03:00.500 --> 00:03:04.090
If you have a larger corpus,
you can instead add-k.

48
00:03:04.090 --> 00:03:09.873
This technique called add-k smoothing
makes the probabilities even smoother.

49
00:03:09.873 --> 00:03:13.011
The formula is similar
to add-one smoothing.

50
00:03:13.011 --> 00:03:17.642
Simply add-k to the numerator
in each possible N-gram in

51
00:03:17.642 --> 00:03:22.941
the denominator where it sums up to
k by the size of the vocabulary.

52
00:03:22.941 --> 00:03:26.598
So k-add smoothing can be
applied to higher order and

53
00:03:26.598 --> 00:03:31.644
grand probabilities as well like
tri-grams, four-grams and beyond.

54
00:03:31.644 --> 00:03:35.833
There are even more advanced smoothing
methods like the Kneser-Ney or

55
00:03:35.833 --> 00:03:36.818
Good-Turing.

56
00:03:36.818 --> 00:03:41.824
Another approach to dealing with N-grams
that do not occur in the corpus is

57
00:03:41.824 --> 00:03:47.076
to use information about (N-1)-grams,
(N-2)-grams, and so on.

58
00:03:47.076 --> 00:03:52.346
With backoff, if N-gram information
is missing, you use (N-1)-gram.

59
00:03:52.346 --> 00:03:56.380
If that's also missing,
you would use (N-2)-gram and so

60
00:03:56.380 --> 00:03:58.978
on until you find nonzero probability.

61
00:03:58.978 --> 00:04:06.291
Using the lower level N-grams i.e
(N-1)-gram, (N-2)-gram down to uni-gram.

62
00:04:06.291 --> 00:04:11.333
It distorts the probability distribution
especially for smaller corporal.

63
00:04:11.333 --> 00:04:16.390
Some probability needs to be discounted
from higher-level N-grams to use it for

64
00:04:16.390 --> 00:04:18.011
lower-level N-grams.

65
00:04:18.011 --> 00:04:21.277
This Katz backoff method uses discounting.

66
00:04:21.277 --> 00:04:27.637
In very large web scale corpus is a method
called Stupid backoff has been effective.

67
00:04:27.637 --> 00:04:31.604
With Stupid backoff,
no probability discounting is applied.

68
00:04:31.604 --> 00:04:35.364
If the higher order N-gram
probability is missing,

69
00:04:35.364 --> 00:04:38.631
the lower order N-gram
probability is used.

70
00:04:38.631 --> 00:04:40.697
Just multiplied by a constant.

71
00:04:40.697 --> 00:04:44.786
A constant of about 0.4 was
experimentally shown to work well.

72
00:04:44.786 --> 00:04:47.542
Let's use backoff on an example.

73
00:04:47.542 --> 00:04:52.171
If you look at this corpus,
the probability of the tri-gram John

74
00:04:52.171 --> 00:04:56.816
drinks chocolate can't be directly
estimated from the corpus.

75
00:04:56.816 --> 00:05:01.704
So the probability of the bygram,
drinks chocolate multiplied by a constant.

76
00:05:01.704 --> 00:05:05.310
In your scenario 0.4,
would be used instead.

77
00:05:05.310 --> 00:05:10.268
An alternative approach to backoff
is to use the linear interpolation

78
00:05:10.268 --> 00:05:12.208
of all orders of N-grams.

79
00:05:12.208 --> 00:05:17.065
That means that you would always
combine the weighted probability

80
00:05:17.065 --> 00:05:20.629
of the N-gram and
-1 gram down to uni-grams.

81
00:05:20.629 --> 00:05:25.620
For example, when calculating the
probability for the tri-gram John drinks

82
00:05:25.620 --> 00:05:30.771
chocolate, you could take 70% of
the estimated probability for tri-gram.

83
00:05:30.771 --> 00:05:35.764
So John drinks chocolate plus 20%
of the estimated probability for

84
00:05:35.764 --> 00:05:38.134
by-gram, drinks chocolate and

85
00:05:38.134 --> 00:05:43.218
10% of the estimated uni-gram
probability of the word chocolate.

86
00:05:43.218 --> 00:05:47.698
More generally, for tri-grams,
you would combine the weighted

87
00:05:47.698 --> 00:05:51.545
probabilities of tri-gram,
by-gram and uni-gram.

88
00:05:51.545 --> 00:05:58.335
You wait all these probabilities
with constants like λ1, λ1 and λ1.

89
00:05:58.335 --> 00:06:01.050
These need to add up to 1.

90
00:06:01.050 --> 00:06:04.450
The lambdas are learned from
the validation parts of the corpus.

91
00:06:04.450 --> 00:06:08.779
You can get them by maximizing
the probability of sentences from

92
00:06:08.779 --> 00:06:10.231
the validation set.

93
00:06:10.231 --> 00:06:15.111
Use a fixed language model trained from
the training part of the corpus to

94
00:06:15.111 --> 00:06:19.272
calculate N-gram probabilities and
optimize the lambdas.

95
00:06:19.272 --> 00:06:25.454
The interpolation can be applied to
general N-grams by using more lambdas.

96
00:06:25.454 --> 00:06:28.014
Now, you're an expert in
N-gram language models.

97
00:06:28.014 --> 00:06:32.168
You know how to create them,
how to handle out of vocabulary words and

98
00:06:32.168 --> 00:06:34.686
how to improve the model with smoothing.

99
00:06:34.686 --> 00:06:37.909
You'll see that they work really
well in the coding exercises,

100
00:06:37.909 --> 00:06:41.021
where you will write your first
program that generates text.