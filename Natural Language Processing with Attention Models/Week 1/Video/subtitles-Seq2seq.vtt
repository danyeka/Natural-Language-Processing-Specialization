WEBVTT

1
00:00:00.000 --> 00:00:02.100
Good to see you again.

2
00:00:02.100 --> 00:00:04.950
You will now learn about
neural machine translation,

3
00:00:04.950 --> 00:00:06.750
and you'll see what
the architecture

4
00:00:06.750 --> 00:00:08.350
of this neural
network looks like.

5
00:00:08.350 --> 00:00:10.080
You will also learn which words

6
00:00:10.080 --> 00:00:11.760
the neural network
is focusing on

7
00:00:11.760 --> 00:00:14.625
when translating from
one language to another.

8
00:00:14.625 --> 00:00:17.085
Let's formalize this task.

9
00:00:17.085 --> 00:00:20.200
To get started on
this week's material,

10
00:00:20.200 --> 00:00:21.410
I'll introduce you to

11
00:00:21.410 --> 00:00:23.270
neural machine
translation along with

12
00:00:23.270 --> 00:00:24.740
the model that was traditionally

13
00:00:24.740 --> 00:00:26.210
used for its implementation.

14
00:00:26.210 --> 00:00:27.860
The seq2seq model.

15
00:00:27.860 --> 00:00:32.180
Then, I'll talk about some
of this models shortcomings

16
00:00:32.180 --> 00:00:35.120
and the solution as they
lead into the model

17
00:00:35.120 --> 00:00:37.505
that you'll be using in
this week's assignments.

18
00:00:37.505 --> 00:00:38.580
Exciting stuff.

19
00:00:38.580 --> 00:00:39.720
Let's go.

20
00:00:39.720 --> 00:00:41.505
In neural machine translation,

21
00:00:41.505 --> 00:00:43.910
you're using an
encoder and a decoder

22
00:00:43.910 --> 00:00:46.625
to translate from one
language to another.

23
00:00:46.625 --> 00:00:48.905
For example, you
could translate,

24
00:00:48.905 --> 00:00:50.630
it's time for tea from English

25
00:00:50.630 --> 00:00:52.665
to French, C'est l'heure du the.

26
00:00:52.665 --> 00:00:56.720
To do this, you could use a
machine translation system

27
00:00:56.720 --> 00:01:00.970
that has LSTMs for both
encoding and decoding.

28
00:01:00.970 --> 00:01:04.190
The traditional seq2seq
model was introduced

29
00:01:04.190 --> 00:01:06.410
by Google in 2014

30
00:01:06.410 --> 00:01:09.050
and it was a revelation
at the time.

31
00:01:09.050 --> 00:01:12.917
Basically, it works by
taking one sequence of items

32
00:01:12.917 --> 00:01:16.905
such as words and its
output, another sequence.

33
00:01:16.905 --> 00:01:19.820
The way this is
done is by mapping

34
00:01:19.820 --> 00:01:22.880
variable length sequences
to a fixed length memory,

35
00:01:22.880 --> 00:01:24.770
which in machine translation,

36
00:01:24.770 --> 00:01:27.350
encodes the overall
meaning of sentences.

37
00:01:27.350 --> 00:01:29.315
For example, you can have

38
00:01:29.315 --> 00:01:31.190
a text of length that varies

39
00:01:31.190 --> 00:01:33.500
and you can encode
it into a vector

40
00:01:33.500 --> 00:01:36.635
or fixed dimension
like 300, for example.

41
00:01:36.635 --> 00:01:38.390
This feature is what's made

42
00:01:38.390 --> 00:01:42.094
this model a powerhouse
for machine translation.

43
00:01:42.094 --> 00:01:45.650
Additionally, the
inputs and outputs

44
00:01:45.650 --> 00:01:47.375
don't need to have
matching lengths,

45
00:01:47.375 --> 00:01:51.320
which is a desirable feature
when translating texts.

46
00:01:51.320 --> 00:01:54.260
Then you might recall the
vanishing and exploding

47
00:01:54.260 --> 00:01:57.740
gradients problems from
earlier in the specialization.

48
00:01:57.740 --> 00:01:59.405
In seq2seq model,

49
00:01:59.405 --> 00:02:04.360
LSTMs and GRUs are typically
used to avoid these problems.

50
00:02:04.360 --> 00:02:07.895
As I mentioned, in
a seq2seq model,

51
00:02:07.895 --> 00:02:10.595
you have an encoder
and a decoder.

52
00:02:10.595 --> 00:02:14.675
The encoder takes
word tokens as input,

53
00:02:14.675 --> 00:02:18.695
and it returns its final
hidden states as outputs.

54
00:02:18.695 --> 00:02:21.860
This hidden state is
used by the decoder to

55
00:02:21.860 --> 00:02:25.570
generate the translated sentence
in the target language.

56
00:02:25.570 --> 00:02:27.140
Before moving on,

57
00:02:27.140 --> 00:02:31.055
let's look closer at the
encoder and decoder.

58
00:02:31.055 --> 00:02:34.895
The encoder typically consists
of an embedding layer

59
00:02:34.895 --> 00:02:38.045
and an LSTM module with
one or more layers.

60
00:02:38.045 --> 00:02:41.630
The embedding layer
transforms words tokenized

61
00:02:41.630 --> 00:02:45.545
first into a vector for
input to the LSTM module.

62
00:02:45.545 --> 00:02:48.080
At each step in the
input sequence,

63
00:02:48.080 --> 00:02:51.635
the LSTM module receives inputs
from the embedding layer,

64
00:02:51.635 --> 00:02:54.500
as well as the hidden states
from the previous step.

65
00:02:54.500 --> 00:02:57.995
The encoder returns the hidden
states of the final step,

66
00:02:57.995 --> 00:02:59.930
shown here as h_4.

67
00:02:59.930 --> 00:03:02.270
This final hidden
state has information

68
00:03:02.270 --> 00:03:03.320
from the whole sentence

69
00:03:03.320 --> 00:03:05.750
and it encodes its
overall meaning.

70
00:03:05.750 --> 00:03:08.300
The decoder is constructed

71
00:03:08.300 --> 00:03:11.705
similarly with an embedding
layer and an LSTM layer.

72
00:03:11.705 --> 00:03:14.180
You use the output
word of a step

73
00:03:14.180 --> 00:03:16.580
as the input word
for the next step.

74
00:03:16.580 --> 00:03:20.660
You also pass the LSTM hidden
state to the next step.

75
00:03:20.660 --> 00:03:22.820
You start the input sequence

76
00:03:22.820 --> 00:03:23.930
where there is start of sequence

77
00:03:23.930 --> 00:03:27.365
token denoted as SOS here.

78
00:03:27.365 --> 00:03:29.040
The first step, C'est,

79
00:03:29.040 --> 00:03:31.845
as the most probable next word.

80
00:03:31.845 --> 00:03:35.690
Then you use C'est as the
input word for the next step

81
00:03:35.690 --> 00:03:37.100
and repeat to generate

82
00:03:37.100 --> 00:03:39.420
the rest of the sentence
l'heure du the.

83
00:03:42.080 --> 00:03:46.460
One major limitation of the
traditional seq2seq model

84
00:03:46.460 --> 00:03:49.730
is what's referred to as
the information bottleneck.

85
00:03:49.730 --> 00:03:52.510
Since seq2seq uses a
fixed length memory

86
00:03:52.510 --> 00:03:53.710
for the hidden states,

87
00:03:53.710 --> 00:03:56.270
long sequences
become problematic.

88
00:03:56.270 --> 00:03:58.255
This is due to the fact

89
00:03:58.255 --> 00:04:01.210
that in traditional
seq2seq models,

90
00:04:01.210 --> 00:04:04.000
only a fixed amount of
information can be passed

91
00:04:04.000 --> 00:04:06.940
from the encoder to
the decoder no matter

92
00:04:06.940 --> 00:04:10.475
how much information is
contained in the input sequence.

93
00:04:10.475 --> 00:04:12.905
The power of seq2seq,

94
00:04:12.905 --> 00:04:14.350
which allows for inputs and

95
00:04:14.350 --> 00:04:16.225
outputs to be different sizes,

96
00:04:16.225 --> 00:04:20.000
becomes not effective when
the input sequence is long.

97
00:04:20.000 --> 00:04:22.960
The result is lower
model performance,

98
00:04:22.960 --> 00:04:26.180
a sequence size increases
and that's no good.

99
00:04:26.180 --> 00:04:28.180
The issue with having one fixed

100
00:04:28.180 --> 00:04:30.160
size encoder hidden states

101
00:04:30.160 --> 00:04:31.840
is that it struggles to compress

102
00:04:31.840 --> 00:04:35.090
longer sequences and it
ends up throttling itself

103
00:04:35.090 --> 00:04:36.545
and punishing the decoder

104
00:04:36.545 --> 00:04:38.975
who only wants to make
a good prediction.

105
00:04:38.975 --> 00:04:42.410
One workaround is to use
the encoder hidden states

106
00:04:42.410 --> 00:04:44.210
for each word instead of trying

107
00:04:44.210 --> 00:04:46.610
to smash it all into
one big vector.

108
00:04:46.610 --> 00:04:50.495
But this model would have flaws
with memory and contexts.

109
00:04:50.495 --> 00:04:53.750
How could you build a time
and memory efficient model

110
00:04:53.750 --> 00:04:57.070
that predicts accurately
from a long sequence?

111
00:04:57.070 --> 00:05:00.620
This becomes possible if the
model has a way to select

112
00:05:00.620 --> 00:05:04.120
and focus on the most important
words at each time step.

113
00:05:04.120 --> 00:05:06.110
You can think of this as giving

114
00:05:06.110 --> 00:05:09.334
the model a new layer to
process this information,

115
00:05:09.334 --> 00:05:12.515
which in the slide
is called attention.

116
00:05:12.515 --> 00:05:14.330
If you provide the information

117
00:05:14.330 --> 00:05:16.490
specific to each input word,

118
00:05:16.490 --> 00:05:18.920
you can give the
model a way to focus

119
00:05:18.920 --> 00:05:20.990
it's attention in
the right place

120
00:05:20.990 --> 00:05:23.590
at each step of the
decoding process.

121
00:05:23.590 --> 00:05:25.335
That is good progress.

122
00:05:25.335 --> 00:05:28.820
Up next, you'll get
a conceptual idea of

123
00:05:28.820 --> 00:05:31.390
what this new layer
is doing and why.

124
00:05:31.390 --> 00:05:34.910
You now have an overview of
neural machine translation,

125
00:05:34.910 --> 00:05:36.890
and you have a rough idea

126
00:05:36.890 --> 00:05:39.245
of what attention
is looking like.

127
00:05:39.245 --> 00:05:42.050
You know which words the
model is focusing on

128
00:05:42.050 --> 00:05:45.960
when translating from one
language to another language.