WEBVTT

1
00:00:00.000 --> 00:00:02.890
I will now show you
how to implement RNNs.

2
00:00:02.890 --> 00:00:05.240
I'll be talking
about scan functions

3
00:00:05.240 --> 00:00:07.840
which are just like
abstract RNNs,

4
00:00:07.840 --> 00:00:10.340
and they allow for
faster computation.

5
00:00:10.340 --> 00:00:14.805
What are scan functions and
how do you implement them?

6
00:00:14.805 --> 00:00:18.380
To do this, I will show you
how the function scan is

7
00:00:18.380 --> 00:00:20.420
implemented in TensorFlow for

8
00:00:20.420 --> 00:00:23.200
computing the forward
pass in RNNs,

9
00:00:23.200 --> 00:00:25.720
the scan function
is designed to take

10
00:00:25.720 --> 00:00:27.860
a function fn and apply

11
00:00:27.860 --> 00:00:29.560
it to all of the elements from

12
00:00:29.560 --> 00:00:32.605
the beginning to the
end in the list elems.

13
00:00:32.605 --> 00:00:35.260
Initializer is an
optional variable that

14
00:00:35.260 --> 00:00:38.355
could be used in the
first computation of fn.

15
00:00:38.355 --> 00:00:40.200
Now take this RNN,

16
00:00:40.200 --> 00:00:43.305
where fn is equivalent to fw,

17
00:00:43.305 --> 00:00:47.865
elems is the list with
all the input x^t,

18
00:00:47.865 --> 00:00:53.550
and the initializer is
the hidden state, h^t_0.

19
00:00:53.550 --> 00:00:56.180
The scan function
first initializes

20
00:00:56.180 --> 00:00:59.970
the hidden state as h^t_0,

21
00:00:59.970 --> 00:01:02.240
and sets the ys which

22
00:01:02.240 --> 00:01:05.125
store the prediction
values as an empty list.

23
00:01:05.125 --> 00:01:09.280
Then for every x in the
list of elements fn is

24
00:01:09.280 --> 00:01:10.740
called with x and

25
00:01:10.740 --> 00:01:13.920
the value of the last
hidden states is arguments.

26
00:01:13.920 --> 00:01:17.380
This for loop computes
every time step of

27
00:01:17.380 --> 00:01:19.020
the RNN and stores

28
00:01:19.020 --> 00:01:21.940
the values of the prediction
in hidden states.

29
00:01:21.940 --> 00:01:24.220
Finally, the function
returns the list of

30
00:01:24.220 --> 00:01:26.720
predictions and the
last hidden state.

31
00:01:26.720 --> 00:01:28.340
You might think this function

32
00:01:28.340 --> 00:01:29.860
is unnecessary because it is

33
00:01:29.860 --> 00:01:31.720
essentially a for loop

34
00:01:31.720 --> 00:01:34.100
through every time
step of the RNN.

35
00:01:34.100 --> 00:01:37.760
However, frameworks like
TensorFlow need this type

36
00:01:37.760 --> 00:01:39.360
of abstraction in order

37
00:01:39.360 --> 00:01:41.335
to perform parallel
computations,

38
00:01:41.335 --> 00:01:43.210
and run on GPUs.

39
00:01:43.210 --> 00:01:45.940
I showed you how the scan
function is defined in

40
00:01:45.940 --> 00:01:48.680
TensorFlow to mimic
how RNNs work.

41
00:01:48.680 --> 00:01:51.620
It is important to know
that these types of

42
00:01:51.620 --> 00:01:54.520
abstractions are needed for
deep learning frameworks,

43
00:01:54.520 --> 00:01:55.700
because they allow them to use

44
00:01:55.700 --> 00:01:58.780
GPUs and compute in parallel.