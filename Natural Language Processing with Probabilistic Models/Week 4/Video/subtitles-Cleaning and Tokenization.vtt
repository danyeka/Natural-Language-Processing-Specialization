WEBVTT

1
00:00:00.000 --> 00:00:03.240
We will now explore
cleaning and tokenization.

2
00:00:03.240 --> 00:00:06.495
I already spoke about this a
little bit in the Course 1,

3
00:00:06.495 --> 00:00:09.480
but this is important to touch
it again for a little bit.

4
00:00:09.480 --> 00:00:11.310
Let's get started. I'll give you

5
00:00:11.310 --> 00:00:15.030
some practical advice on how
to clean a corpus and split

6
00:00:15.030 --> 00:00:17.265
it into words or more accurately

7
00:00:17.265 --> 00:00:20.730
tokens through a process
known as tokenization.

8
00:00:20.730 --> 00:00:24.540
In Course 1, I previously
spoke about data preparation,

9
00:00:24.540 --> 00:00:26.415
cleaning, and tokenization,

10
00:00:26.415 --> 00:00:28.965
but now I'll go into
some more details.

11
00:00:28.965 --> 00:00:30.930
First, you should
consider the words of

12
00:00:30.930 --> 00:00:32.970
your corpus as case insensitive.

13
00:00:32.970 --> 00:00:35.160
For instance, the
word 'The' should be

14
00:00:35.160 --> 00:00:37.820
represented identically
regardless of its case,

15
00:00:37.820 --> 00:00:40.545
for example, whether it
begins with a capital T,

16
00:00:40.545 --> 00:00:42.970
say at the beginning of
the sentence or not.

17
00:00:42.970 --> 00:00:45.370
You can do this by
converting the corpus

18
00:00:45.370 --> 00:00:48.310
to either all lowercase
or all uppercase.

19
00:00:48.310 --> 00:00:51.220
Secondly, you should
handle punctuation.

20
00:00:51.220 --> 00:00:53.665
You could, for instance,
represents all interrupting

21
00:00:53.665 --> 00:00:56.405
punctuation marks
such as full stops,

22
00:00:56.405 --> 00:00:58.360
commas, and question marks as

23
00:00:58.360 --> 00:01:00.775
a single special word
of the vocabulary.

24
00:01:00.775 --> 00:01:03.370
You could ignore non-interrupting
punctuation marks

25
00:01:03.370 --> 00:01:04.825
such as quotation marks.

26
00:01:04.825 --> 00:01:07.180
You could collapse
multi-sign marks such as

27
00:01:07.180 --> 00:01:10.525
triple question marks into
a single mark and so on.

28
00:01:10.525 --> 00:01:13.105
Next, you want to
handle numbers.

29
00:01:13.105 --> 00:01:15.340
For example, if numbers do not

30
00:01:15.340 --> 00:01:17.935
carry an important
meaning in your use case,

31
00:01:17.935 --> 00:01:19.885
you could drop all
of the numbers.

32
00:01:19.885 --> 00:01:21.940
However, numbers may have

33
00:01:21.940 --> 00:01:25.155
significant meaning that is
relevant to your use case.

34
00:01:25.155 --> 00:01:29.570
For instance, 3.14 is
the number for Pi,

35
00:01:29.570 --> 00:01:31.490
90210 is the name of

36
00:01:31.490 --> 00:01:33.080
a television show and

37
00:01:33.080 --> 00:01:35.990
the area code for Beverly
Hills, California.

38
00:01:35.990 --> 00:01:39.360
You can keep these numbers
in your corpora as this.

39
00:01:39.360 --> 00:01:41.630
If your corpora has
many unique numbers

40
00:01:41.630 --> 00:01:43.310
such as many area codes,

41
00:01:43.310 --> 00:01:45.845
you may find that it makes
more sense to replace

42
00:01:45.845 --> 00:01:49.830
all the numbers with a special
token such as &lt;NUMBER&gt;.

43
00:01:49.830 --> 00:01:51.620
This allows the
model to know that

44
00:01:51.620 --> 00:01:54.650
the important thing is
that it's a number instead

45
00:01:54.650 --> 00:01:55.820
of trying to distinguish

46
00:01:55.820 --> 00:01:59.620
between 90210 and other area
codes or phone numbers.

47
00:01:59.620 --> 00:02:02.020
You also need to handle
special characters

48
00:02:02.020 --> 00:02:03.920
such as mathematical symbols,

49
00:02:03.920 --> 00:02:07.100
currency symbols, section
and paragraph signs,

50
00:02:07.100 --> 00:02:09.665
online markup signs, and so on.

51
00:02:09.665 --> 00:02:11.815
It's usually safe to drop them.

52
00:02:11.815 --> 00:02:14.240
Finally, and especially
if you're working on

53
00:02:14.240 --> 00:02:15.950
modern corpus of user inputs

54
00:02:15.950 --> 00:02:18.050
such as tweets or
consumer reviews,

55
00:02:18.050 --> 00:02:20.390
you should handle
special words such

56
00:02:20.390 --> 00:02:22.940
as emojis and hashtags,

57
00:02:22.940 --> 00:02:25.175
#nlp, depending on

58
00:02:25.175 --> 00:02:26.600
if and how you want your model

59
00:02:26.600 --> 00:02:27.920
to confirm meanings to them.

60
00:02:27.920 --> 00:02:29.060
You could, for example,

61
00:02:29.060 --> 00:02:31.070
consider that each emoji or

62
00:02:31.070 --> 00:02:34.505
hashtag should be considered
as an individual word.

63
00:02:34.505 --> 00:02:36.740
I'll give a basic
example in Python to

64
00:02:36.740 --> 00:02:39.365
demonstrate a few of
these recommendations.

65
00:02:39.365 --> 00:02:41.615
The corpus in this example is,

66
00:02:41.615 --> 00:02:44.230
"Who loves word
embeddings in 2020?

67
00:02:44.230 --> 00:02:49.190
I do!!!" With an emoji
punctuation marks and the number.

68
00:02:49.190 --> 00:02:52.070
Here's the code to import and
initialize the libraries.

69
00:02:52.070 --> 00:02:54.560
I'm using the
popular NLTK library

70
00:02:54.560 --> 00:02:56.360
to perform tokenization.

71
00:02:56.360 --> 00:02:58.940
It has a smart
tokenization module named

72
00:02:58.940 --> 00:03:02.495
'Punkt' to handle common
special uses of punctuation.

73
00:03:02.495 --> 00:03:06.450
For example, it knows that
full stops which are periods,

74
00:03:06.450 --> 00:03:08.720
and abbreviations, and
middle names do not

75
00:03:08.720 --> 00:03:11.315
signify the end of a sentence.

76
00:03:11.315 --> 00:03:13.880
I'm also using the
emoji library just

77
00:03:13.880 --> 00:03:16.595
to give you an idea of how
you could handle emojis.

78
00:03:16.595 --> 00:03:18.760
Next comes the actual logic.

79
00:03:18.760 --> 00:03:21.515
First, using
irregular expression,

80
00:03:21.515 --> 00:03:23.300
I'm collapsing all interrupting

81
00:03:23.300 --> 00:03:26.375
punctuation signs and replacing
them with a full stop.

82
00:03:26.375 --> 00:03:27.935
The outcome here is,

83
00:03:27.935 --> 00:03:30.455
"Who loves word
embeddings in 2020?

84
00:03:30.455 --> 00:03:32.450
I do." With the question and

85
00:03:32.450 --> 00:03:35.350
exclamation marks replaced
with a single full stop.

86
00:03:35.350 --> 00:03:37.500
Next, I'm using NLTK word

87
00:03:37.500 --> 00:03:39.320
tokenize function to split

88
00:03:39.320 --> 00:03:41.645
this string into an
array of tokens.

89
00:03:41.645 --> 00:03:44.750
As you can see, the
punctuation signs have been

90
00:03:44.750 --> 00:03:47.270
separated as individual tokens

91
00:03:47.270 --> 00:03:49.025
including the quotation marks.

92
00:03:49.025 --> 00:03:51.800
Finally, using a
list comprehension,

93
00:03:51.800 --> 00:03:54.320
I'm keeping tokens and
converting them to

94
00:03:54.320 --> 00:03:56.030
lowercase if they are one of

95
00:03:56.030 --> 00:03:58.030
the following: alphabetical,

96
00:03:58.030 --> 00:03:59.760
a full stop, so previously in

97
00:03:59.760 --> 00:04:02.550
interrupting punctuation
mark, and emoji.

98
00:04:02.550 --> 00:04:04.370
This gets rid of numbers such as

99
00:04:04.370 --> 00:04:07.420
2020 and unknown
special characters.

100
00:04:07.420 --> 00:04:09.045
This is the resulting array.

101
00:04:09.045 --> 00:04:11.690
You can now use this array
to extract center words and

102
00:04:11.690 --> 00:04:13.415
their surrounding context words

103
00:04:13.415 --> 00:04:16.330
which we'll dive into
in more detail next.

104
00:04:16.330 --> 00:04:19.235
After looking at cleaning
and tokenization,

105
00:04:19.235 --> 00:04:21.260
we are ready to move
on to the next part

106
00:04:21.260 --> 00:04:23.495
of the continuous
bag-of-words model.

107
00:04:23.495 --> 00:04:24.995
In the next video,

108
00:04:24.995 --> 00:04:27.770
we will explore the sliding
Window which you can

109
00:04:27.770 --> 00:04:30.785
think of as a Window going
over the text corpus.

110
00:04:30.785 --> 00:04:34.000
We will talk more about
it. See you there.