WEBVTT

1
00:00:00.000 --> 00:00:03.960
Previously, I spoke about
dense and ReLU layers.

2
00:00:03.960 --> 00:00:06.925
There are two other layers
I want to introduce you to.

3
00:00:06.925 --> 00:00:09.000
Embedding, and mean layers.

4
00:00:09.000 --> 00:00:11.780
These will come in
handy. Let's dive in.

5
00:00:11.780 --> 00:00:14.480
Going forward, I will introduce

6
00:00:14.480 --> 00:00:17.540
embedding layers and
explain why you'll need to

7
00:00:17.540 --> 00:00:20.100
include layers such
as the mean layer in

8
00:00:20.100 --> 00:00:23.900
serial models directly
after your embedding layer.

9
00:00:23.900 --> 00:00:26.780
Let's now dive into
the embedding layer.

10
00:00:26.780 --> 00:00:29.020
In NLP, you usually have

11
00:00:29.020 --> 00:00:32.475
a set of unique words
called the vocabulary.

12
00:00:32.475 --> 00:00:34.790
An embedding layer
takes an index

13
00:00:34.790 --> 00:00:37.330
assigned to each word
from your vocabulary,

14
00:00:37.330 --> 00:00:39.510
and maps it to a representation

15
00:00:39.510 --> 00:00:41.990
of that word with a
determined dimension.

16
00:00:41.990 --> 00:00:45.275
In this example, embedding
of size equal to 2.

17
00:00:45.275 --> 00:00:48.870
For instance, the embedding
layer in this example will

18
00:00:48.870 --> 00:00:52.615
return a vector equal to 0.020,

19
00:00:52.615 --> 00:00:55.810
and 0.006 for the word I.

20
00:00:55.810 --> 00:01:02.430
And negative 0.009 and
0.050 for the word NLP.

21
00:01:02.430 --> 00:01:06.150
Every value from this parts
of the table is trainable.

22
00:01:06.150 --> 00:01:09.110
When using an embedding
layer in your model,

23
00:01:09.110 --> 00:01:11.330
you'll learn the representation

24
00:01:11.330 --> 00:01:14.450
that gives the best
performance for your NLP task.

25
00:01:14.450 --> 00:01:16.950
For the embedding
layer in your model,

26
00:01:16.950 --> 00:01:20.490
you'd have to learn a
matrix of weights of size

27
00:01:20.490 --> 00:01:22.830
equal to your vocabulary times

28
00:01:22.830 --> 00:01:24.690
the dimension of the embedding.

29
00:01:24.690 --> 00:01:26.790
The size of the
embedding could be

30
00:01:26.790 --> 00:01:29.850
treated as a hyperparameter
in your model.

31
00:01:29.850 --> 00:01:32.370
With this layer, your
model can learn or

32
00:01:32.370 --> 00:01:35.240
improve the word embeddings
for your NLP task,

33
00:01:35.240 --> 00:01:38.770
like it improves the weight
matrices on each layer.

34
00:01:38.770 --> 00:01:43.310
The embedding layer is able
to map words to embeddings.

35
00:01:43.310 --> 00:01:45.630
If you had a series of words

36
00:01:45.630 --> 00:01:48.415
like a tweet that
says, "I'm happy",

37
00:01:48.415 --> 00:01:50.670
the embedding layer
will map each of

38
00:01:50.670 --> 00:01:53.420
those words to their
corresponding embedding,

39
00:01:53.420 --> 00:01:56.265
and return a matrix
of word embeddings.

40
00:01:56.265 --> 00:01:59.700
If you had padded vectors
representing your tweets,

41
00:01:59.700 --> 00:02:02.450
you could unroll
this matrix and feed

42
00:02:02.450 --> 00:02:05.710
its values to the next layer
on the neural network.

43
00:02:05.710 --> 00:02:07.610
But in doing this, you might

44
00:02:07.610 --> 00:02:10.350
end up with a lot of
parameters to train.

45
00:02:10.350 --> 00:02:13.010
As an alternative,
you could just

46
00:02:13.010 --> 00:02:15.605
take the mean of each
feature from the embedding,

47
00:02:15.605 --> 00:02:17.930
and that's exactly what
the mean layer does.

48
00:02:17.930 --> 00:02:20.490
After the mean layer you'll
have the same number

49
00:02:20.490 --> 00:02:23.190
of features as your
embedding sites,

50
00:02:23.190 --> 00:02:26.490
even for sequences of
texts that are very long.

51
00:02:26.490 --> 00:02:28.670
Note that this
layer doesn't have

52
00:02:28.670 --> 00:02:31.270
any trainable
parameters because it's

53
00:02:31.270 --> 00:02:35.035
only computing the mean
of the word embeddings.

54
00:02:35.035 --> 00:02:37.750
At this point you're
familiar with what

55
00:02:37.750 --> 00:02:41.670
embedding layers and mean
layers are, and how they work.

56
00:02:41.670 --> 00:02:44.530
An important takeaway
here is that using

57
00:02:44.530 --> 00:02:47.490
an embedding layer in
your model allows you to

58
00:02:47.490 --> 00:02:49.550
learn a good representation of

59
00:02:49.550 --> 00:02:54.010
your vocabulary for the specific
task you're working on.

60
00:02:54.010 --> 00:02:58.310
As the mean layer takes a
matrix of embeddings and

61
00:02:58.310 --> 00:03:00.510
returns a vector representation

62
00:03:00.510 --> 00:03:03.380
for a set of words like tweets.

63
00:03:03.380 --> 00:03:05.835
You have four layers
in your toolkit.

64
00:03:05.835 --> 00:03:07.500
Dense, ReLU,

65
00:03:07.500 --> 00:03:09.290
embedding, and mean.

66
00:03:09.290 --> 00:03:11.830
These are more than enough
to lay the foundations

67
00:03:11.830 --> 00:03:15.240
for you to start building
your own neural network.