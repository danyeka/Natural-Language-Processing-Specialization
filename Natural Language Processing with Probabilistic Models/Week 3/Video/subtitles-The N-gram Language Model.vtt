WEBVTT

1
00:00:00.000 --> 00:00:02.430
I'm going to show you
how to populate and

2
00:00:02.430 --> 00:00:05.070
create a count matrix,
which for example,

3
00:00:05.070 --> 00:00:06.525
in the case of migrants,

4
00:00:06.525 --> 00:00:08.010
tells you how many times

5
00:00:08.010 --> 00:00:10.755
each word is followed
by every other word.

6
00:00:10.755 --> 00:00:12.540
I will show you how to calculate

7
00:00:12.540 --> 00:00:14.865
probabilities and
even how to avoid

8
00:00:14.865 --> 00:00:17.310
underflow when multiplying
a lot of numbers

9
00:00:17.310 --> 00:00:20.785
between zero and one.
Let's get started.

10
00:00:20.785 --> 00:00:25.020
First, you will process the
corpus into a count matrix.

11
00:00:25.020 --> 00:00:26.460
This captures the number of

12
00:00:26.460 --> 00:00:28.890
occurrences of relative n-grams.

13
00:00:28.890 --> 00:00:32.360
Next, you will transform
the count matrix into

14
00:00:32.360 --> 00:00:34.880
a probability matrix
that contains

15
00:00:34.880 --> 00:00:37.595
information about the
conditional probability

16
00:00:37.595 --> 00:00:38.840
of the n-grams,

17
00:00:38.840 --> 00:00:40.430
then you will relate

18
00:00:40.430 --> 00:00:43.075
the probability matrix
to the language model.

19
00:00:43.075 --> 00:00:45.530
I will also show you how to deal

20
00:00:45.530 --> 00:00:47.900
with technical issues
that arise from

21
00:00:47.900 --> 00:00:49.520
multiplying a lots of

22
00:00:49.520 --> 00:00:53.125
small numbers when calculating
the sentence probability.

23
00:00:53.125 --> 00:00:56.240
The last section, briefly
describes how to use

24
00:00:56.240 --> 00:00:58.010
the n-gram language model to

25
00:00:58.010 --> 00:01:00.440
generate new sentences
from scratch.

26
00:01:00.440 --> 00:01:02.755
Let's start with a count matrix.

27
00:01:02.755 --> 00:01:05.360
Here's a reminder
of the formula for

28
00:01:05.360 --> 00:01:08.240
testing the conditional
probability of an n-gram,

29
00:01:08.240 --> 00:01:13.015
lowercase n denotes where the
n-gram ends in a sentence.

30
00:01:13.015 --> 00:01:15.190
The count matrix captures

31
00:01:15.190 --> 00:01:18.895
the numerator for all n-grams
appearing in the corpus.

32
00:01:18.895 --> 00:01:22.375
All unique corpus and minus
one grams make up the rows,

33
00:01:22.375 --> 00:01:25.840
and all unique words of the
corpus makeup the columns.

34
00:01:25.840 --> 00:01:28.770
Now look at the count
matrix of a bigram model.

35
00:01:28.770 --> 00:01:31.560
For the corpus I
study, I learned,

36
00:01:31.560 --> 00:01:34.510
the rows represents
the first word of

37
00:01:34.510 --> 00:01:35.950
the bigram and the columns

38
00:01:35.950 --> 00:01:38.140
represent the second
word of the bigram.

39
00:01:38.140 --> 00:01:40.165
For bigram study I,

40
00:01:40.165 --> 00:01:43.030
you need to find a row
with the word study,

41
00:01:43.030 --> 00:01:44.840
and a column with the word I.

42
00:01:44.840 --> 00:01:47.790
This bigram appears just
once in the corpus.

43
00:01:47.790 --> 00:01:49.510
The whole count matrix can be

44
00:01:49.510 --> 00:01:52.390
treated in a single pass
through the corpus.

45
00:01:52.390 --> 00:01:55.370
You can do that by reading
through the corpus with

46
00:01:55.370 --> 00:01:57.320
a sliding window composed

47
00:01:57.320 --> 00:01:59.800
of two words to
represent your bigram.

48
00:01:59.800 --> 00:02:01.730
For each bigram you find,

49
00:02:01.730 --> 00:02:04.820
you increase the value in
the count matrix by one.

50
00:02:04.820 --> 00:02:07.520
Let's move on to the
probability matrix.

51
00:02:07.520 --> 00:02:10.310
Now that you've used the
count matrix to provide

52
00:02:10.310 --> 00:02:13.955
your numerator for the
n-gram probability formula,

53
00:02:13.955 --> 00:02:16.140
it's time to get
the denominator.

54
00:02:16.140 --> 00:02:18.140
First, update the count matrix

55
00:02:18.140 --> 00:02:20.450
by calculating the
sum for each row,

56
00:02:20.450 --> 00:02:22.750
then normalize each cell.

57
00:02:22.750 --> 00:02:24.575
You can do that by dividing

58
00:02:24.575 --> 00:02:27.385
each cell by the
corresponding row sum.

59
00:02:27.385 --> 00:02:30.750
The row sum is equivalent to
your count of the n minus

60
00:02:30.750 --> 00:02:34.540
one gram prefixes from the
formula's denominator.

61
00:02:34.540 --> 00:02:36.425
This will always be true,

62
00:02:36.425 --> 00:02:39.095
since the n minus
one gram prefix

63
00:02:39.095 --> 00:02:41.630
is always followed by some word.

64
00:02:41.630 --> 00:02:44.525
If the prefix was at the
end of the sentence,

65
00:02:44.525 --> 00:02:47.630
it is now followed by the
end of the sentence token.

66
00:02:47.630 --> 00:02:51.455
Let's creates a probability
matrix from an example.

67
00:02:51.455 --> 00:02:55.670
First, calculate the sum of
bigram counts for each row,

68
00:02:55.670 --> 00:02:58.435
then divide each
cell by the row sum.

69
00:02:58.435 --> 00:03:00.120
Let's look at one example.

70
00:03:00.120 --> 00:03:02.700
The prefix I is
followed by study or

71
00:03:02.700 --> 00:03:05.655
by learn as you can see
in the count matrix.

72
00:03:05.655 --> 00:03:07.549
If you add up those
two instances,

73
00:03:07.549 --> 00:03:10.250
you will see that the
word I appears twice.

74
00:03:10.250 --> 00:03:12.905
Now let's go back to
the probability matrix.

75
00:03:12.905 --> 00:03:16.160
You can see that the
probability of I study is

76
00:03:16.160 --> 00:03:20.260
one half and the probability
of I learn is also one half.

77
00:03:20.260 --> 00:03:23.860
The next step is to connect
the probability matrix with

78
00:03:23.860 --> 00:03:25.750
your definition of
the language model

79
00:03:25.750 --> 00:03:27.185
from this week's overview.

80
00:03:27.185 --> 00:03:31.060
The language model can now
be a simple script that uses

81
00:03:31.060 --> 00:03:32.950
the probability matrix to

82
00:03:32.950 --> 00:03:35.860
estimate the probability
of a given sentence.

83
00:03:35.860 --> 00:03:37.960
It estimates the probability

84
00:03:37.960 --> 00:03:40.285
by splitting the sentence into

85
00:03:40.285 --> 00:03:42.790
a series of n-grams and then

86
00:03:42.790 --> 00:03:46.355
finding their probability
in the probability matrix.

87
00:03:46.355 --> 00:03:49.435
Alternatively, the
language model

88
00:03:49.435 --> 00:03:51.460
can predict the next elements of

89
00:03:51.460 --> 00:03:53.620
a sequence by extracting

90
00:03:53.620 --> 00:03:57.035
the last n minus one gram
from the end of a sequence.

91
00:03:57.035 --> 00:03:59.500
After that, the
language model finds

92
00:03:59.500 --> 00:04:02.590
the corresponding grow in
the probability matrix,

93
00:04:02.590 --> 00:04:05.120
and returns the word with
the highest probability.

94
00:04:05.120 --> 00:04:08.150
Using the probability matrix
from the previous slide,

95
00:04:08.150 --> 00:04:10.880
find the probability of
the sentence I learn.

96
00:04:10.880 --> 00:04:15.505
You take 1 times 0.5 times
1, which equals 0.5.

97
00:04:15.505 --> 00:04:17.690
You have a 50 percent
chance of seeing

98
00:04:17.690 --> 00:04:20.375
the sentence I learn
next in your corpus.

99
00:04:20.375 --> 00:04:22.550
That's cool. There are

100
00:04:22.550 --> 00:04:25.340
a few loose ends in the
language model implementation.

101
00:04:25.340 --> 00:04:28.760
Let's discuss them. The census
probability calculation

102
00:04:28.760 --> 00:04:32.420
requires multiplication of
a lots of small numbers.

103
00:04:32.420 --> 00:04:34.940
In fact, all of
the probabilities

104
00:04:34.940 --> 00:04:37.825
fall in the interval 0 to 1,

105
00:04:37.825 --> 00:04:40.025
and multiplying
many probabilities

106
00:04:40.025 --> 00:04:42.575
brings the risk of
numerical underflow.

107
00:04:42.575 --> 00:04:45.110
You may remember this
from previous modules.

108
00:04:45.110 --> 00:04:47.740
All you need to know
is that computers have

109
00:04:47.740 --> 00:04:50.445
difficulty storing very
small decimal numbers,

110
00:04:50.445 --> 00:04:52.500
and this can end
up causing errors.

111
00:04:52.500 --> 00:04:54.175
If you have an
opportunity to store

112
00:04:54.175 --> 00:04:56.410
a larger number
instead, you should.

113
00:04:56.410 --> 00:04:59.605
You may recall the mathematical
trick for solving this,

114
00:04:59.605 --> 00:05:01.240
where you wrote the product of

115
00:05:01.240 --> 00:05:03.880
terms as the sum of other terms,

116
00:05:03.880 --> 00:05:06.230
the logarithm of a product.

117
00:05:06.230 --> 00:05:08.380
One interesting application of

118
00:05:08.380 --> 00:05:11.380
language models is text
generation from scratch,

119
00:05:11.380 --> 00:05:13.120
or using a small hint.

120
00:05:13.120 --> 00:05:14.830
For example, the algorithm

121
00:05:14.830 --> 00:05:17.420
chooses brackets S Lyn to start.

122
00:05:17.420 --> 00:05:21.105
Next, it chooses a
bigram with Lyn.

123
00:05:21.105 --> 00:05:23.540
In this case, Lyn,drinks.

124
00:05:23.540 --> 00:05:26.335
Then is chooses drinks,tea,

125
00:05:26.335 --> 00:05:30.100
and finally, it chooses to
end the sentence there.

126
00:05:30.100 --> 00:05:32.030
This happens for all bigrams

127
00:05:32.030 --> 00:05:33.965
starting with tea
in your corpus.

128
00:05:33.965 --> 00:05:36.755
This is how the algorithm
accomplishes this.

129
00:05:36.755 --> 00:05:40.805
First, it randomly chooses
among all bigrams,

130
00:05:40.805 --> 00:05:44.965
starting with the start of
sentence symbol brackets S,

131
00:05:44.965 --> 00:05:47.610
based on the bigram probability.

132
00:05:47.610 --> 00:05:51.320
That means the bigrams
with higher values

133
00:05:51.320 --> 00:05:55.015
in the probability matrix are
more likely to be chosen.

134
00:05:55.015 --> 00:05:58.850
Next, the algorithm
chooses a new bigram

135
00:05:58.850 --> 00:06:00.710
at random from the bigrams

136
00:06:00.710 --> 00:06:03.155
beginning with the
previously chosen word.

137
00:06:03.155 --> 00:06:06.710
Then, this bigram is
added to your sentence.

138
00:06:06.710 --> 00:06:08.990
The algorithm continues on like

139
00:06:08.990 --> 00:06:11.495
this until the end
sentence token,

140
00:06:11.495 --> 00:06:13.975
brackets backslash S is chosen.

141
00:06:13.975 --> 00:06:15.750
As you might have guessed,

142
00:06:15.750 --> 00:06:18.485
this is done by randomly
choosing a bigram

143
00:06:18.485 --> 00:06:21.440
that starts with the
previous word and ends

144
00:06:21.440 --> 00:06:25.130
with the backslash S. Now
you know how to calculate

145
00:06:25.130 --> 00:06:27.110
n-gram probabilities
from a corpus

146
00:06:27.110 --> 00:06:29.495
so we can build your
own language model.

147
00:06:29.495 --> 00:06:33.360
Next, I'll show you
how to evaluate it.