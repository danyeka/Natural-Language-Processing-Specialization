WEBVTT

1
00:00:00.000 --> 00:00:02.565
In this video, I'm
going to show you

2
00:00:02.565 --> 00:00:04.830
how you can use the
probability matrix,

3
00:00:04.830 --> 00:00:07.080
the one that you learned
in the previous video.

4
00:00:07.080 --> 00:00:09.900
I'll show you how you
can use it to create

5
00:00:09.900 --> 00:00:11.580
a path so that you can assign

6
00:00:11.580 --> 00:00:13.710
a parts of speech
tag to every word.

7
00:00:13.710 --> 00:00:16.065
Let me show you how
you can do this.

8
00:00:16.065 --> 00:00:18.540
The backward pass is the last of

9
00:00:18.540 --> 00:00:20.910
three steps of the
Viterbi algorithm,

10
00:00:20.910 --> 00:00:23.610
where you will retrieve the
most likely sequence of

11
00:00:23.610 --> 00:00:26.865
parts of speech tags for your
given sequence of words.

12
00:00:26.865 --> 00:00:30.480
By now, you've populated
the matrices C and D.

13
00:00:30.480 --> 00:00:32.400
Now you just have
to extract the path

14
00:00:32.400 --> 00:00:34.500
through your graph
from the matrix D,

15
00:00:34.500 --> 00:00:36.150
which represents the sequence of

16
00:00:36.150 --> 00:00:39.824
hidden states that's most
likely generated our sequence

17
00:00:39.824 --> 00:00:42.570
where at one all
the way towards K.

18
00:00:42.570 --> 00:00:47.270
First calculate the index
of the entry C_i,K with

19
00:00:47.270 --> 00:00:49.460
the highest probability
in the last column

20
00:00:49.460 --> 00:00:52.850
of C. The probability
at this index is

21
00:00:52.850 --> 00:00:56.090
the probability of the
most likely sequence of

22
00:00:56.090 --> 00:00:59.780
hidden states generating the
given sequence of words.

23
00:00:59.780 --> 00:01:03.200
You use this index as to
traverse backwards through

24
00:01:03.200 --> 00:01:05.240
the matrix D to reconstruct

25
00:01:05.240 --> 00:01:07.430
the sequence of parts
of speech tags.

26
00:01:07.430 --> 00:01:10.100
First, calculate the
index of the entry

27
00:01:10.100 --> 00:01:14.285
CIK with the highest probability
in the last column of C.

28
00:01:14.285 --> 00:01:17.900
The probability at this
index is the probability of

29
00:01:17.900 --> 00:01:19.400
the most likely sequence of

30
00:01:19.400 --> 00:01:23.030
hidden states generating the
given sequence of words.

31
00:01:23.030 --> 00:01:26.390
You use this index s to
traverse backwards through

32
00:01:26.390 --> 00:01:28.430
the matrix D to reconstruct

33
00:01:28.430 --> 00:01:30.755
the sequence of parts
of speech tags.

34
00:01:30.755 --> 00:01:34.190
Let's look at a simple
example for a matrix D. For

35
00:01:34.190 --> 00:01:35.690
a model with four states and

36
00:01:35.690 --> 00:01:37.760
a given word sequence
of length five.

37
00:01:37.760 --> 00:01:40.505
The matrix D stores
all the labels

38
00:01:40.505 --> 00:01:41.630
of the hidden states you've

39
00:01:41.630 --> 00:01:43.415
traversed in the forward path.

40
00:01:43.415 --> 00:01:45.440
If you're going back
through the states,

41
00:01:45.440 --> 00:01:48.395
starting with the path that
has the highest probability,

42
00:01:48.395 --> 00:01:51.170
you effectively got the
most likely sequence of

43
00:01:51.170 --> 00:01:54.140
hidden states or
parts of speech tags.

44
00:01:54.140 --> 00:01:55.625
You start by looking up

45
00:01:55.625 --> 00:01:57.770
the entry with the
highest probability in

46
00:01:57.770 --> 00:02:00.050
the last row of the matrix C and

47
00:02:00.050 --> 00:02:02.765
extract the index
s of that entry.

48
00:02:02.765 --> 00:02:04.790
Let's look at the matrix C with

49
00:02:04.790 --> 00:02:06.260
some probabilities
you might have

50
00:02:06.260 --> 00:02:08.435
computed in the forward pass.

51
00:02:08.435 --> 00:02:12.210
You now want to calculate
the index s of the entry

52
00:02:12.210 --> 00:02:16.085
in the last row of C which
has the highest probability.

53
00:02:16.085 --> 00:02:18.740
The entry with the
highest probability is

54
00:02:18.740 --> 00:02:21.830
the first one with 0.01.

55
00:02:21.830 --> 00:02:23.715
Written as a formula,

56
00:02:23.715 --> 00:02:26.845
s is the argmax i of C_i,K,

57
00:02:26.845 --> 00:02:29.280
which in this case
is equal to 1.

58
00:02:29.280 --> 00:02:31.880
That index represents the
last hidden state you

59
00:02:31.880 --> 00:02:34.970
traversed when you
observe the word W_5.

60
00:02:34.970 --> 00:02:37.175
The most likely state of word

61
00:02:37.175 --> 00:02:40.360
W_5 is the t_1 parts
of speech tag.

62
00:02:40.360 --> 00:02:42.330
You add t_1 to the end of

63
00:02:42.330 --> 00:02:45.060
the sequence and look
up the next index in D,

64
00:02:45.060 --> 00:02:47.025
which tells you
where you came from.

65
00:02:47.025 --> 00:02:49.260
This next index is three.

66
00:02:49.260 --> 00:02:52.100
Now move on to the fourth
word in the sequence,

67
00:02:52.100 --> 00:02:54.590
which means you're now looking
at the fourth column in

68
00:02:54.590 --> 00:02:56.180
the matrix D. To

69
00:02:56.180 --> 00:02:58.460
decide which row of
the matrix to look at,

70
00:02:58.460 --> 00:03:00.335
recall that in matrix D,

71
00:03:00.335 --> 00:03:03.380
column 5, the top row
holds the value three,

72
00:03:03.380 --> 00:03:05.390
where three represents
the previous state

73
00:03:05.390 --> 00:03:07.315
with the highest probability,

74
00:03:07.315 --> 00:03:11.610
so t_3 is the most likely
state for word number 4.

75
00:03:11.610 --> 00:03:14.070
You would associate
word number 4,

76
00:03:14.070 --> 00:03:15.730
with the state number 3,

77
00:03:15.730 --> 00:03:18.470
which is the parts of
speech labeled number 3.

78
00:03:18.470 --> 00:03:20.600
You can keep walking
left to each column of

79
00:03:20.600 --> 00:03:23.720
the matrix D. Since the
value stored in column 4,

80
00:03:23.720 --> 00:03:25.730
row 3 is the number 1,

81
00:03:25.730 --> 00:03:27.680
you can assign the
parts of speech tag,

82
00:03:27.680 --> 00:03:31.195
t_1 to the previous
word, word number 3.

83
00:03:31.195 --> 00:03:34.955
Also, when you go
left to column 3,

84
00:03:34.955 --> 00:03:36.635
you will look for row 1,

85
00:03:36.635 --> 00:03:38.225
representing state 1,

86
00:03:38.225 --> 00:03:40.280
which you see
highlighted in green.

87
00:03:40.280 --> 00:03:42.540
The value stored in column 3,

88
00:03:42.540 --> 00:03:44.410
row 1 is three.

89
00:03:44.410 --> 00:03:47.060
This means that the
previous word has parts of

90
00:03:47.060 --> 00:03:49.775
speech tag number 3
associated with it.

91
00:03:49.775 --> 00:03:53.530
So associated word number 2
with parts of speech tag 3.

92
00:03:53.530 --> 00:03:57.680
Now walk left one column
in the matrix to column 2.

93
00:03:57.680 --> 00:03:58.940
Since the value stored in

94
00:03:58.940 --> 00:04:01.010
the green cell in
column 3 is three,

95
00:04:01.010 --> 00:04:03.095
go to row 3 of column 2.

96
00:04:03.095 --> 00:04:06.915
In column 2, the value
stored in row 3 is two.

97
00:04:06.915 --> 00:04:08.720
This means that for word 1,

98
00:04:08.720 --> 00:04:12.280
the most likely state is
parts of speech tag number 2.

99
00:04:12.280 --> 00:04:13.690
The algorithm stops as you

100
00:04:13.690 --> 00:04:15.430
have arrived at the start token

101
00:04:15.430 --> 00:04:16.720
with the value stored in

102
00:04:16.720 --> 00:04:19.295
the second row of the
first column being zero.

103
00:04:19.295 --> 00:04:21.790
The sequence of t, i that
you have recovered in

104
00:04:21.790 --> 00:04:23.980
the backward pass
is the sequence of

105
00:04:23.980 --> 00:04:26.635
parts of speech tags with
the highest probability.

106
00:04:26.635 --> 00:04:28.480
Before you go, there are

107
00:04:28.480 --> 00:04:31.720
two implementations specific
issues to be aware of.

108
00:04:31.720 --> 00:04:34.000
When you implement
the Viterbi algorithm

109
00:04:34.000 --> 00:04:35.665
in the programming assignments,

110
00:04:35.665 --> 00:04:37.540
be careful with the indices.

111
00:04:37.540 --> 00:04:39.730
As list or matrix indices in

112
00:04:39.730 --> 00:04:42.600
Python starts with
zero instead of one.

113
00:04:42.600 --> 00:04:45.880
Another implementation
specific issue is when you

114
00:04:45.880 --> 00:04:49.170
multiply many very small
numbers like probabilities,

115
00:04:49.170 --> 00:04:51.460
this will lead to
numerical issues.

116
00:04:51.460 --> 00:04:53.915
You should use log
probabilities instead,

117
00:04:53.915 --> 00:04:56.725
where numbers are summed
instead of multiplying.

118
00:04:56.725 --> 00:04:58.490
Don't worry. I'll be

119
00:04:58.490 --> 00:05:00.815
revisiting this concept
later in the course.

120
00:05:00.815 --> 00:05:03.245
Congratulations on
finishing this week.

121
00:05:03.245 --> 00:05:05.795
You now know about
the Viterbi algorithm

122
00:05:05.795 --> 00:05:07.145
and specifically,

123
00:05:07.145 --> 00:05:09.755
you've used this for
part-of-speech tagging.

124
00:05:09.755 --> 00:05:12.260
You can use part-of-speech
tagging in search,

125
00:05:12.260 --> 00:05:13.655
in machine translation,

126
00:05:13.655 --> 00:05:16.505
in speech recognition,
and also in parsing.

127
00:05:16.505 --> 00:05:18.125
Next week you're going to learn

128
00:05:18.125 --> 00:05:19.865
about a different type of task,

129
00:05:19.865 --> 00:05:23.670
which is known as
n-gram language models.